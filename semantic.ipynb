{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95e78dea",
   "metadata": {},
   "source": [
    "# Chapter 4.1: RAG: Contextual Grounding via Semantic Search\n",
    "\n",
    "In Chapters 1-3, we explored the fundamentals of AI-assisted programming: task taxonomy, mental models, prompt engineering, and environment setup. We learned that language models have powerful capabilities, but they also have critical limitations—most notably, they're constrained by their training data and context windows.\n",
    "\n",
    "Consider this scenario: You're working with a large codebase containing thousands of files. You need help understanding a specific module's functionality. The entire codebase won't fit in the model's context window, and even if it did, the model wasn't trained on your specific code. How do you provide the AI with the right context to assist you effectively?\n",
    "\n",
    "This is where **Retrieval-Augmented Generation (RAG)** shines—a technique that fundamentally transforms how we work with language models by dynamically providing them with relevant context retrieved from external sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ee5eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Configure your server URL here\n",
    "SERVER_HOST = 'http://ollama.cs.wallawalla.edu:11434'\n",
    "client = ollama.Client(host=SERVER_HOST)\n",
    "\n",
    "def call_ollama(prompt, model=\"cs450\", **options):\n",
    "    \"\"\"\n",
    "    Send a prompt to the Ollama API.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The prompt to send\n",
    "        model (str): Model name to use\n",
    "        **options: Additional model parameters (temperature, top_k, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        str: The model's response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            options=options\n",
    "        )\n",
    "        return response['response']\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def call_ollama_full(prompt, model=\"cs450\", **options):\n",
    "    try:\n",
    "        response = client.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            options=options\n",
    "        )\n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674fbbef",
   "metadata": {},
   "source": [
    "### The Limitations LLMs Face\n",
    "\n",
    "**1. Knowledge Cutoff**: Models are frozen in time, trained on data up to a specific date. They don't know about:\n",
    "- Your proprietary codebase\n",
    "- Recent library updates or API changes  \n",
    "- Company-specific conventions and patterns\n",
    "- Project-specific documentation\n",
    "\n",
    "**2. Context Window Constraints**: Even with modern large context windows (32K, 128K tokens), you often can't fit:\n",
    "- Entire codebases (millions of lines)\n",
    "- Complete documentation sets\n",
    "- Historical project discussions and decisions\n",
    "- All relevant code dependencies\n",
    "\n",
    "**3. Hallucination Risk**: When models don't have relevant information, they may:\n",
    "- Generate plausible-sounding but incorrect information\n",
    "- Invent APIs or functions that don't exist\n",
    "- Misremember details from their training data\n",
    "\n",
    "### The RAG Solution\n",
    "\n",
    "RAG addresses these limitations through a three-step process:\n",
    "\n",
    "1. **Index**: Pre-process and store your documents in a searchable format\n",
    "2. **Retrieve**: Find the most relevant documents for a given query\n",
    "3. **Generate**: Provide retrieved context to the model along with your prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d208307a",
   "metadata": {},
   "source": [
    "## Semantic Search: The Foundation of RAG\n",
    "\n",
    "RAG's power comes from **semantic search**—the ability to find documents based on meaning rather than just keyword matching. This is fundamentally different from traditional search.\n",
    "\n",
    "### Traditional vs. Semantic Search\n",
    "\n",
    "**Traditional (Keyword) Search:**\n",
    "- Query: \"python function sorting\"\n",
    "- Matches documents containing these exact words\n",
    "- Misses: \"def bubble_sort(arr):\" (no word \"function\")\n",
    "- Misses: \"organizing data in ascending order\" (different terminology)\n",
    "\n",
    "**Semantic Search:**\n",
    "- Query: \"python function sorting\"\n",
    "- Understands the *meaning* and *intent*\n",
    "- Finds: Implementation of sorting functions\n",
    "- Finds: Explanations using different terminology\n",
    "- Finds: Related concepts (algorithms, data structures)\n",
    "\n",
    "### How Semantic Search Works\n",
    "\n",
    "Semantic search relies on **embeddings**—vector representations that capture meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95eac30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Embeddings\n",
      "============================================================\n",
      "\n",
      "Text: The cat sat on the mat\n",
      "Embedding dimensions: 768\n",
      "First 5 values: [1.004204511642456, 1.3216490745544434, -2.537903070449829, -0.5848085880279541, 1.1537489891052246]\n",
      "\n",
      "Text: A feline rested on a rug\n",
      "Embedding dimensions: 768\n",
      "First 5 values: [0.989753246307373, 1.0614591836929321, -3.260462760925293, -1.8742859363555908, 0.5663155317306519]\n",
      "\n",
      "Text: Python is a programming language\n",
      "Embedding dimensions: 768\n",
      "First 5 values: [0.2254875749349594, 1.77924644947052, -2.091967821121216, -1.2990589141845703, 1.6509016752243042]\n",
      "\n",
      "Text: The dog ran in the park\n",
      "Embedding dimensions: 768\n",
      "First 5 values: [0.19359147548675537, 0.8133468627929688, -3.2348642349243164, -0.14081451296806335, 0.2091991901397705]\n"
     ]
    }
   ],
   "source": [
    "def explore_embeddings():\n",
    "    \"\"\"Understand what embeddings are and how they work.\"\"\"\n",
    "    \n",
    "    # Generate embeddings for similar and different texts\n",
    "    texts = [\n",
    "        \"The cat sat on the mat\",\n",
    "        \"A feline rested on a rug\",\n",
    "        \"Python is a programming language\",\n",
    "        \"The dog ran in the park\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Generating Embeddings\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        response = client.embeddings(\n",
    "            model='nomic-embed-text',\n",
    "            prompt=text\n",
    "        )\n",
    "        embeddings.append(response['embedding'])\n",
    "        print(f\"\\nText: {text}\")\n",
    "        print(f\"Embedding dimensions: {len(response['embedding'])}\")\n",
    "        print(f\"First 5 values: {response['embedding'][:5]}\")\n",
    "    \n",
    "    return texts, embeddings\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    explore_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9565c39",
   "metadata": {},
   "source": [
    "**Key Insights:**\n",
    "\n",
    "1. **High-Dimensional Vectors**: Embeddings are typically 384-1536 dimensions\n",
    "2. **Semantic Encoding**: Similar meanings → similar vectors\n",
    "3. **Distance Metrics**: Closer vectors → more similar meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fd36b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "programming-genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
