{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69babadd",
   "metadata": {},
   "source": [
    "# Metacoding\n",
    "\n",
    "## Teaching Machines to Understand Programs\n",
    "\n",
    "What if we could teach computers to understand code the way they understand human language? What if a program could recognize that `for i in range(n)` and `while i < n` are semantically related, even though they use different syntax? This is possible with **code embeddings**, where we transform the discrete, symbolic nature of programming languages into continuous mathematical spaces that machines can reason about.\n",
    "\n",
    "## The Vector Space of Code\n",
    "\n",
    "The key insight is that code has structure that generic text models miss. Operators matter more than variable names. Indentation carries meaning. A single character change (`<` to `>`) can completely reverse logic. Code-specific embeddings learn these patterns from millions of examples, giving us representations that reflect how programmers actually think about code.\n",
    "\n",
    "### What Are Code Embeddings?\n",
    "\n",
    "Recall that an **embedding** is a learned mapping from discrete objects (like words, sentences, or code) into continuous vector spaces. Instead of treating code as raw text, we represent it as points in a mathematical space where:\n",
    "\n",
    "- **Distance** captures semantic similarity\n",
    "- **Direction** encodes meaningful relationships\n",
    "- **Arithmetic** reveals hidden patterns\n",
    "\n",
    "### Why Code Embeddings?\n",
    "\n",
    "Code has semantic meaning just like natural language:\n",
    "\n",
    "- Two functions might solve the same problem differently\n",
    "- Comments and variable names carry intent\n",
    "- Code structure reveals algorithmic approach\n",
    "\n",
    "For example, these Python functions do the same thing:\n",
    "\n",
    "```python\n",
    "def sum_list_v1(numbers):\n",
    "    total = 0\n",
    "    for n in numbers:\n",
    "        total += n\n",
    "    return total\n",
    "\n",
    "def sum_list_v2(lst):\n",
    "    return sum(lst)\n",
    "```\n",
    "A good embedding should recognize their similarity!\n",
    "\n",
    "### Code Embedding Models\n",
    "\n",
    "Modern code embeddings come from models trained on millions of repositories.\n",
    "\n",
    "UniXcoder (2022) is a unified model for multiple languages which  significantly outperforms older models and benefits from recent advances in transformers. It is currently one of the best open-source models for code understanding tasks.\n",
    "\n",
    "Let's start with a simple example using a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52ddc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity (iterative sum vs built-in sum): 0.6089\n",
      "Similarity (sum vs multiply):               0.8638\n",
      "\n",
      "The model recognizes the two sum functions are semantically similar!\n",
      "despite different implementations (similarity: 0.6089)\n"
     ]
    }
   ],
   "source": [
    "# Using UniXcoder for code embeddings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/unixcoder-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/unixcoder-base\")\n",
    "\n",
    "def embed_code(code_snippet):\n",
    "        \"\"\"Generate embedding for text using mean pooling.\"\"\"\n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", \n",
    "                               truncation=True, max_length=512,\n",
    "                               padding=True)\n",
    "        \n",
    "        # Generate embedding\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        # Mean pooling over all tokens (best practice for embeddings)\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        \n",
    "        # Weighted average using attention mask\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        embedding = (sum_embeddings / sum_mask).squeeze().numpy()\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "def cosine_similarity(emb1, emb2):\n",
    "    \"\"\"Compute cosine similarity between two embeddings.\"\"\"\n",
    "    # Ensure numpy arrays\n",
    "    if torch.is_tensor(emb1):\n",
    "        emb1 = emb1.cpu().numpy()\n",
    "    if torch.is_tensor(emb2):\n",
    "        emb2 = emb2.cpu().numpy()\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    dot_product = np.dot(emb1, emb2)\n",
    "    norm1 = np.linalg.norm(emb1)\n",
    "    norm2 = np.linalg.norm(emb2)\n",
    "    \n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "# Compare two functionally equivalent sum functions\n",
    "emb1 = embed_code(\"\"\"\n",
    "def change_list(numbers):\n",
    "    total = 0\n",
    "    for n in numbers:\n",
    "        total += n\n",
    "    return total\n",
    "\"\"\")\n",
    "\n",
    "emb2 = embed_code(\"\"\"\n",
    "def sum_list(lst):\n",
    "    return sum(lst)\n",
    "\"\"\")\n",
    "\n",
    "emb3 = embed_code(\"\"\"\n",
    "def multiply_list(numbers):\n",
    "    result = 1\n",
    "    for n in numbers:\n",
    "        result *= n\n",
    "    return result\n",
    "\"\"\")\n",
    "\n",
    "similarity_same = cosine_similarity(emb1, emb2)\n",
    "similarity_different = cosine_similarity(emb1, emb3)\n",
    "\n",
    "print(f\"Similarity (iterative sum vs built-in sum): {similarity_same:.4f}\")\n",
    "print(f\"Similarity (sum vs multiply):               {similarity_different:.4f}\")\n",
    "print(f\"\\nThe model recognizes the two sum functions are semantically similar!\")\n",
    "print(f\"despite different implementations (similarity: {similarity_same:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaf0c85",
   "metadata": {},
   "source": [
    "It's also interesting to note the difference between sum and multiply is significant, even though we only changed one operator (and the name and intialization value for `result`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12fa3a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity to Original:\n",
      "\n",
      "0.9597  with_typing\n",
      "0.9493  different_name\n",
      "0.9301  renamed_vars\n",
      "0.8477  different_logic\n",
      "0.3574  unrelated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test variations of the same function\n",
    "original = \"\"\"def add_numbers(a, b):\n",
    "    '''Add two numbers together.'''\n",
    "    return a + b\"\"\"\n",
    "\n",
    "variations = {\n",
    "    \"renamed_vars\": \"\"\"def add_numbers(x, y):\n",
    "    '''Add two numbers together.'''\n",
    "    return x + y\"\"\",\n",
    "    \n",
    "    \"different_name\": \"\"\"def sum_values(a, b):\n",
    "    '''Add two numbers together.'''\n",
    "    return a + b\"\"\",\n",
    "    \n",
    "    \"with_typing\": \"\"\"def add_numbers(a: int, b: int) -> int:\n",
    "    '''Add two numbers together.'''\n",
    "    return a + b\"\"\",\n",
    "    \n",
    "    \"different_logic\": \"\"\"def multiply_numbers(a, b):\n",
    "    '''Multiply two numbers together.'''\n",
    "    return a * b\"\"\",\n",
    "    \n",
    "    \"unrelated\": \"\"\"def send_email(recipient, message):\n",
    "    '''Send an email to recipient.'''\n",
    "    print(f\"Sending to {recipient}: {message}\")\"\"\"\n",
    "}\n",
    "\n",
    "# Compare all variations to original\n",
    "base_emb = embed_code(original).reshape(1, -1)\n",
    "\n",
    "print(\"Similarity to Original:\\n\")\n",
    "results = []\n",
    "for name, code in variations.items():\n",
    "    emb = embed_code(code).reshape(1, -1)\n",
    "    sim = cosine_similarity(base_emb, emb)[0][0]\n",
    "    results.append((name, sim))\n",
    "\n",
    "# Sort by similarity (highest first)\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for name, sim in results:\n",
    "    print(f\"{sim:.4f}  {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "309ad005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "UniXcoder Operator Sensitivity Test\n",
      "======================================================================\n",
      "\n",
      "Original Function:\n",
      "def validate_age(age):\n",
      "    '''Check if age meets minimum requirement.'''\n",
      "    if age < 18:\n",
      "        return False\n",
      "    return True\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Similarity to Original (sorted high to low):\n",
      "======================================================================\n",
      "Test Case            Similarity   Changes                       \n",
      "----------------------------------------------------------------------\n",
      "one_operator         0.9970       1 operator: < → >             \n",
      "two_operators        0.9942       2 operators: < → >, flip returns\n",
      "comment_only         0.9715       0 operators (added comment)   \n",
      "three_operators      0.9623       3 operators: < → >=, 18 → 21, flip returns\n",
      "cosmetic_only        0.9424       0 operators (cosmetic: age → user_age)\n",
      "\n",
      "======================================================================\n",
      "Detailed Analysis\n",
      "======================================================================\n",
      "\n",
      "one_operator:\n",
      "  Similarity: 0.9970\n",
      "  Changes: 1 operator: < → >\n",
      "  Description: Changed: age > 18 (completely different logic)\n",
      "\n",
      "two_operators:\n",
      "  Similarity: 0.9942\n",
      "  Changes: 2 operators: < → >, flip returns\n",
      "  Description: Changed: age > 18, swapped True/False\n",
      "\n",
      "comment_only:\n",
      "  Similarity: 0.9715\n",
      "  Changes: 0 operators (added comment)\n",
      "  Description: Only added comment\n",
      "\n",
      "three_operators:\n",
      "  Similarity: 0.9623\n",
      "  Changes: 3 operators: < → >=, 18 → 21, flip returns\n",
      "  Description: Changed: age >= 21, swapped True/False\n",
      "\n",
      "cosmetic_only:\n",
      "  Similarity: 0.9424\n",
      "  Changes: 0 operators (cosmetic: age → user_age)\n",
      "  Description: Only variable name changed\n"
     ]
    }
   ],
   "source": [
    "# Original function\n",
    "original = \"\"\"def validate_age(age):\n",
    "    '''Check if age meets minimum requirement.'''\n",
    "    if age < 18:\n",
    "        return False\n",
    "    return True\"\"\"\n",
    "\n",
    "# Test cases: progressively change operators\n",
    "test_cases = {\n",
    "    \"original\": {\n",
    "        \"code\": \"\"\"def validate_age(age):\n",
    "    '''Check if age meets minimum requirement.'''\n",
    "    if age < 18:\n",
    "        return False\n",
    "    return True\"\"\",\n",
    "        \"changes\": \"None (baseline)\",\n",
    "        \"description\": \"Original: age < 18\"\n",
    "    },\n",
    "    \n",
    "    \"one_operator\": {\n",
    "        \"code\": \"\"\"def validate_age(age):\n",
    "    '''Check if age meets minimum requirement.'''\n",
    "    if age > 18:\n",
    "        return False\n",
    "    return True\"\"\",\n",
    "        \"changes\": \"1 operator: < → >\",\n",
    "        \"description\": \"Changed: age > 18 (completely different logic)\"\n",
    "    },\n",
    "    \n",
    "    \"two_operators\": {\n",
    "        \"code\": \"\"\"def validate_age(age):\n",
    "    '''Check if age meets minimum requirement.'''\n",
    "    if age > 18:\n",
    "        return True\n",
    "    return False\"\"\",\n",
    "        \"changes\": \"2 operators: < → >, flip returns\",\n",
    "        \"description\": \"Changed: age > 18, swapped True/False\"\n",
    "    },\n",
    "    \n",
    "    \"three_operators\": {\n",
    "        \"code\": \"\"\"def validate_age(age):\n",
    "    '''Check if age meets minimum requirement.'''\n",
    "    if age >= 21:\n",
    "        return True\n",
    "    return False\"\"\",\n",
    "        \"changes\": \"3 operators: < → >=, 18 → 21, flip returns\",\n",
    "        \"description\": \"Changed: age >= 21, swapped True/False\"\n",
    "    },\n",
    "    \n",
    "    \"cosmetic_only\": {\n",
    "        \"code\": \"\"\"def validate_age(user_age):\n",
    "    '''Check if age meets minimum requirement.'''\n",
    "    if user_age < 18:\n",
    "        return False\n",
    "    return True\"\"\",\n",
    "        \"changes\": \"0 operators (cosmetic: age → user_age)\",\n",
    "        \"description\": \"Only variable name changed\"\n",
    "    },\n",
    "    \n",
    "    \"comment_only\": {\n",
    "        \"code\": \"\"\"def validate_age(age):\n",
    "    '''Check if age meets minimum requirement.'''\n",
    "    # Checking minimum age limit\n",
    "    if age < 18:\n",
    "        return False\n",
    "    return True\"\"\",\n",
    "        \"changes\": \"0 operators (added comment)\",\n",
    "        \"description\": \"Only added comment\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"UniXcoder Operator Sensitivity Test\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nOriginal Function:\")\n",
    "print(original)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = {}\n",
    "for name, test in test_cases.items():\n",
    "    embeddings[name] = embed_code(test['code']).reshape(1, -1)\n",
    "\n",
    "# Calculate similarities to original\n",
    "base = embeddings[\"original\"]\n",
    "results = []\n",
    "\n",
    "for name, test in test_cases.items():\n",
    "    if name != \"original\":\n",
    "        sim = cosine_similarity(base, embeddings[name])[0][0]\n",
    "        results.append({\n",
    "            'name': name,\n",
    "            'similarity': sim,\n",
    "            'changes': test['changes'],\n",
    "            'description': test['description']\n",
    "        })\n",
    "\n",
    "# Sort by similarity (highest first)\n",
    "results.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "print(\"\\nSimilarity to Original (sorted high to low):\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Test Case':<20} {'Similarity':<12} {'Changes':<30}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"{r['name']:<20} {r['similarity']:<12.4f} {r['changes']:<30}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Detailed Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"\\n{r['name']}:\")\n",
    "    print(f\"  Similarity: {r['similarity']:.4f}\")\n",
    "    print(f\"  Changes: {r['changes']}\")\n",
    "    print(f\"  Description: {r['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd98a67",
   "metadata": {},
   "source": [
    "## Understanding Variable Names\n",
    "\n",
    "Here's something subtle: UniXcoder doesn't just ignore variable names as cosmetic details. It understands they carry semantic meaning. Consider these variations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211414a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Why Variable Names Matter to UniXcoder\n",
      "======================================================================\n",
      "\n",
      "Original: def check(x): return x < 10\n",
      "\n",
      "Comparing variable name changes:\n",
      "----------------------------------------------------------------------\n",
      "short_rename         (var=y              ) similarity: 0.8852\n",
      "long_rename          (var=value          ) similarity: 0.8973\n",
      "very_long_rename     (var=input_value    ) similarity: 0.8535\n",
      "semantic_rename      (var=threshold      ) similarity: 0.7618\n",
      "\n",
      "======================================================================\n",
      "Insight: Variable Names Carry Semantic Information!\n",
      "======================================================================\n",
      "\n",
      "UniXcoder doesn't just see variables as placeholders. It recognizes:\n",
      "- 'age' suggests age-related logic\n",
      "- 'user_age' suggests user-specific age handling\n",
      "- 'threshold' suggests boundary checking\n",
      "- 'x' is generic\n",
      "\n",
      "When you change variable names, you're potentially changing the\n",
      "*semantic context* of the code, which UniXcoder picks up on!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Why Variable Names Matter to UniXcoder\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test: Same logic, different variable name lengths\n",
    "test_cases = {\n",
    "    \"original\": \"def check(x):\\n    return x < 10\",\n",
    "    \"short_rename\": \"def check(y):\\n    return y < 10\",\n",
    "    \"long_rename\": \"def check(value):\\n    return value < 10\",\n",
    "    \"very_long_rename\": \"def check(input_value):\\n    return input_value < 10\",\n",
    "    \"semantic_rename\": \"def check(threshold):\\n    return threshold < 10\",\n",
    "}\n",
    "\n",
    "print(\"\\nOriginal: def check(x): return x < 10\")\n",
    "print(\"\\nComparing variable name changes:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "base = embed_code(test_cases[\"original\"]).reshape(1, -1)\n",
    "\n",
    "for name, code in test_cases.items():\n",
    "    if name != \"original\":\n",
    "        emb = embed_code(code).reshape(1, -1)\n",
    "        sim = cosine_similarity(base, emb)[0][0]\n",
    "        var_name = code.split('(')[1].split(')')[0]\n",
    "        print(f\"{name:20} (var={var_name:15}) similarity: {sim:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10351e96",
   "metadata": {},
   "source": [
    "**Insight**: Variable Names Carry Semantic Information!\n",
    "\n",
    "UniXcoder doesn't just see variables as placeholders. It recognizes:\n",
    "- 'age' suggests age-related logic\n",
    "- 'user_age' suggests user-specific age handling\n",
    "- 'threshold' suggests boundary checking\n",
    "- 'x' is just generic\n",
    "\n",
    "When you change variable names, you're potentially changing the\n",
    "*semantic context* of the code, which UniXcoder picks up on!\n",
    "\n",
    "Good developers choose meaningful variable names, and UniXcoder learned to recognize this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac58b921",
   "metadata": {},
   "source": [
    "The short rename (x to y) barely changes the embedding—both are generic mathematical variables. But renaming to `age` noticeably reduces similarity. Why? Because `age` adds semantic context. The function isn't just checking if a number is less than 10; it's checking if an age meets some threshold. That's meaningful information that changes how we understand the code.\n",
    "\n",
    "This is actually desirable behavior. If you're searching for \"age validation functions\", you want functions that use the variable name `age` to rank higher than functions using `x`. The variable name is a signal about the function's purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6917b3",
   "metadata": {},
   "source": [
    "## Comparing Code-Specific vs Generic Embeddings\n",
    "\n",
    "Let's run a direct comparison. We'll use UniXcoder and a generic text embedding model (nomic-embed-text) on the same code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c57607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "UniXcoder vs Generic Embedding Comparison\n",
      "======================================================================\n",
      "UniXcoder similarity (< → >): 0.9970\n",
      "Generic similarity (< → >):   0.9995\n",
      "Difference:                   0.0025\n",
      "\n",
      "Which model better recognizes the logical change?\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "ollama_client = ollama.Client(host='http://ollama.cs.wallawalla.edu:11434')\n",
    "\n",
    "def get_ollama_embedding(text):\n",
    "    response = ollama_client.embeddings(\n",
    "        model='nomic-embed-text',\n",
    "        prompt=text\n",
    "    )\n",
    "    return response['embedding']\n",
    "test_cases = {\n",
    "    \"original\": {\n",
    "        \"code\": \"\"\"def validate_age(age):\n",
    "    '''Check if age meets minimum requirement.'''\n",
    "    if age < 18:\n",
    "        return False\n",
    "    return True\"\"\",\n",
    "        \"changes\": \"None (baseline)\",\n",
    "        \"description\": \"Original: age < 18\"\n",
    "    },\n",
    "    \n",
    "    \"one_operator\": {\n",
    "        \"code\": \"\"\"def validate_age(age):\n",
    "    '''Check if age meets minimum requirement.'''\n",
    "    if age > 18:\n",
    "        return False\n",
    "    return True\"\"\",\n",
    "        \"changes\": \"1 operator: < → >\",\n",
    "        \"description\": \"Changed: age > 18 (completely different logic)\"\n",
    "    },\n",
    "}\n",
    "# Compare UniXcoder vs Generic for one operator change\n",
    "original_unix = embed_code(original).reshape(1, -1)\n",
    "one_op_unix = embed_code(test_cases[\"one_operator\"][\"code\"]).reshape(1, -1)\n",
    "\n",
    "original_gen = [get_ollama_embedding(original)]\n",
    "one_op_gen = [get_ollama_embedding(test_cases[\"one_operator\"][\"code\"])]\n",
    "\n",
    "unix_sim = cosine_similarity(original_unix, one_op_unix)[0][0]\n",
    "gen_sim = cosine_similarity(original_gen, one_op_gen)[0][0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"UniXcoder vs Generic Embedding Comparison\")\n",
    "print(\"=\"*70)\n",
    "print(f\"UniXcoder similarity (< → >): {unix_sim:.4f}\")\n",
    "print(f\"Generic similarity (< → >):   {gen_sim:.4f}\")\n",
    "print(f\"Difference:                   {abs(unix_sim - gen_sim):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabeeefb",
   "metadata": {},
   "source": [
    "UniXcoder has `0.0025` less similarity than the generic `nomic-embed-text` model, and so clearly it better recognizes the single **atomic** logical change to the code as expected!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd45b4d",
   "metadata": {},
   "source": [
    "## Building a Code Search Engine\n",
    "\n",
    "Now let's build something practical: a semantic code search engine that finds relevant functions even when they don't share keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d267d352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Similarity: 0.4529\n",
      "   def sum_list(items): return sum(items)\n",
      "\n",
      "2. Similarity: 0.3800\n",
      "   def multiply(x, y): return x * y\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CodeSearchEngine:\n",
    "    def __init__(self):\n",
    "        self.code_snippets = []\n",
    "        self.embeddings = []\n",
    "    \n",
    "    def index(self, code_snippet):\n",
    "        \"\"\"Add code to the search index.\"\"\"\n",
    "        embedding = embed_code(code_snippet)\n",
    "        self.code_snippets.append(code_snippet)\n",
    "        self.embeddings.append(embedding)\n",
    "    \n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"Find most similar code snippets.\"\"\"\n",
    "        query_emb = embed_code(query)\n",
    "        \n",
    "        similarities = [\n",
    "            cosine_similarity(query_emb, emb) \n",
    "            for emb in self.embeddings\n",
    "        ]\n",
    "        \n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'code': self.code_snippets[idx],\n",
    "                'similarity': similarities[idx]\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Build a small code repository\n",
    "search_engine = CodeSearchEngine()\n",
    "\n",
    "search_engine.index(\"def add(a, b): return a + b\")\n",
    "search_engine.index(\"def multiply(x, y): return x * y\")\n",
    "search_engine.index(\"def read_file(path): return open(path).read()\")\n",
    "search_engine.index(\"def sum_list(items): return sum(items)\")\n",
    "\n",
    "# Search for \"function that adds numbers\"\n",
    "results = search_engine.search(\"combine two numbers\", top_k=2)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. Similarity: {result['similarity']:.4f}\")\n",
    "    print(f\"   {result['code']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819a28ac",
   "metadata": {},
   "source": [
    "Notice how the search finds `multiply` and `sum_list` as the most relevant results, even though our query doesn't contain the word \"add\" or \"sum\"!\n",
    "\n",
    "## Code Vector Arithmetic\n",
    "\n",
    "Here's where things get truly magical. Remember the famous word2vec example: `king - man + woman = queen`? Code embeddings exhibit similar algebraic properties!\n",
    "\n",
    "If embeddings capture semantic relationships, then vector arithmetic should reveal patterns:\n",
    "\n",
    "- `sorting_function - recursion + iteration ≈ iterative_sort`\n",
    "- `python_function - python + javascript ≈ javascript_function`\n",
    "- `vulnerable_code - vulnerability ≈ safe_code`\n",
    "\n",
    "Let's implement and test this. The model should identify the iterative factorial as the best match! This demonstrates that embeddings capture **conceptual transformations** like \"converting recursion to iteration.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c3dd5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analogy: recursive : iterative :: factorial_recursive : ?\n",
      "\n",
      "Score: 0.4351\n",
      "def fibonacci(n):\n",
      "    if n <= 1: return n\n",
      "    return fibonacci(n-1) + fibonacci(n-2)\n"
     ]
    }
   ],
   "source": [
    "def code_analogy(a, b, c, candidates, top_k=1):\n",
    "    \"\"\"\n",
    "    Solve: a is to b as c is to ?\n",
    "    (i.e., find d such that a - b ≈ c - d)\n",
    "    \"\"\"\n",
    "    emb_a = embed_code(a)\n",
    "    emb_b = embed_code(b)\n",
    "    emb_c = embed_code(c)\n",
    "    \n",
    "    # Compute target vector: c + (a - b)\n",
    "    target = emb_c + (emb_a - emb_b)\n",
    "    \n",
    "    # Find closest candidate\n",
    "    similarities = []\n",
    "    for candidate in candidates:\n",
    "        emb_d = embed_code(candidate)\n",
    "        sim = cosine_similarity(target, emb_d)\n",
    "        similarities.append((candidate, sim))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Example: \"recursive\" is to \"iterative\" as \"factorial_recursive\" is to ?\n",
    "recursive = \"def factorial(n): return 1 if n <= 1 else n * factorial(n-1)\"\n",
    "iterative = \"def count(n):\\n  i = 0\\n  while i < n: i += 1\"\n",
    "\n",
    "fact_recursive = \"def fact(n):\\n  if n == 0: return 1\\n  return n * fact(n-1)\"\n",
    "\n",
    "\n",
    "candidates = [\n",
    "    \"\"\"def fact_iter(n):\n",
    "    result = 1\n",
    "    for i in range(1, n+1): result *= i\n",
    "    return result\"\"\",\n",
    "    \n",
    "    \"\"\"def fibonacci(n):\n",
    "    if n <= 1: return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\"\"\",\n",
    "]\n",
    "\n",
    "print(\"Analogy: recursive : iterative :: factorial_recursive : ?\")\n",
    "results = code_analogy(recursive, iterative, fact_recursive, candidates)\n",
    "\n",
    "for code, score in results:\n",
    "    print(f\"\\nScore: {score:.4f}\")\n",
    "    print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94046081",
   "metadata": {},
   "source": [
    "This shows something very interesting! The model is actually picking `fibonacci` because it's structurally most similar to the recursive factorial pattern, even though semantically we wanted the iterative version. This teaches an important lesson: embeddings capture patterns they were trained on, and sometimes structural similarity dominates semantic transformations.\n",
    "\n",
    "We can use **multiple examples** to learn the transformation better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdd3f0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Improved analogy with multiple examples:\n",
      "Score: 0.7618\n",
      "def fact_iter(n):\n",
      "    result = 1\n",
      "    for i in range(1, n+1): result *= i\n",
      "    return result\n"
     ]
    }
   ],
   "source": [
    "def improved_analogy(source_pairs, query, candidates, top_k=1):\n",
    "    \"\"\"\n",
    "    Learn transformation from multiple (source, target) pairs.\n",
    "    More robust than single-example analogy.\n",
    "    \"\"\"\n",
    "    # Compute average transformation vector\n",
    "    transform_vectors = []\n",
    "    for src, tgt in source_pairs:\n",
    "        emb_src = embed_code(src)\n",
    "        emb_tgt = embed_code(tgt)\n",
    "        transform_vectors.append(emb_tgt - emb_src)\n",
    "    \n",
    "    # Average transformation (recursive → iterative)\n",
    "    avg_transform = np.mean([v.numpy() if torch.is_tensor(v) else v \n",
    "                            for v in transform_vectors], axis=0)\n",
    "    \n",
    "    # Apply to query\n",
    "    emb_query = embed_code(query)\n",
    "    if torch.is_tensor(emb_query):\n",
    "        emb_query = emb_query.numpy()\n",
    "    \n",
    "    target = emb_query + avg_transform\n",
    "    \n",
    "    # Find best match\n",
    "    similarities = []\n",
    "    for candidate in candidates:\n",
    "        emb_cand = embed_code(candidate)\n",
    "        sim = cosine_similarity(target, emb_cand)\n",
    "        similarities.append((candidate, sim))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Provide multiple examples of recursive→iterative transformation\n",
    "recursive_iterative_pairs = [\n",
    "    (\"\"\"def sum_rec(n):\n",
    "        if n == 0: return 0\n",
    "        return n + sum_rec(n-1)\"\"\",\n",
    "     \"\"\"def sum_iter(n):\n",
    "        total = 0\n",
    "        for i in range(n+1): total += i\n",
    "        return total\"\"\"),\n",
    "    \n",
    "    (\"\"\"def count_rec(n):\n",
    "        if n == 0: return\n",
    "        count_rec(n-1)\n",
    "        print(n)\"\"\",\n",
    "     \"\"\"def count_iter(n):\n",
    "        for i in range(1, n+1):\n",
    "            print(i)\"\"\"),\n",
    "]\n",
    "\n",
    "query = \"\"\"def factorial_rec(n):\n",
    "    if n <= 1: return 1\n",
    "    return n * factorial_rec(n-1)\"\"\"\n",
    "\n",
    "print(\"\\nImproved analogy with multiple examples:\")\n",
    "results = improved_analogy(recursive_iterative_pairs, query, candidates)\n",
    "for code, score in results:\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4c900c",
   "metadata": {},
   "source": [
    "And now we get the correct result!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d49553d",
   "metadata": {},
   "source": [
    "# The Multiple-Exemplar Approach: Learning Robust Transformations\n",
    "\n",
    "The multiple-exemplar approach addresses a fundamental limitation of single-example analogies. Let's break this down with both intuition and implementation.\n",
    "\n",
    "## The Problem with Single Examples\n",
    "\n",
    "When we do `a - b + c`, we're assuming that the vector `a - b` captures a **pure, general transformation**. But in reality:\n",
    "\n",
    "```python\n",
    "# What we hope:\n",
    "recursive_func - iterative_func = \"pure recursion concept\"\n",
    "\n",
    "# What we actually get:\n",
    "recursive_func - iterative_func = \"recursion\" + \"factorial-ness\" + \"variable names\" + noise\n",
    "```\n",
    "\n",
    "A single example is **contaminated** by the specific details of those two functions!\n",
    "\n",
    "By averaging transformations across multiple examples, we **cancel out the noise** and extract the common pattern.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8105f250",
   "metadata": {},
   "source": [
    "## Why This Works: The Math\n",
    "\n",
    "The key insight is **linear superposition**. If embeddings capture semantic features linearly:\n",
    "\n",
    "```python\n",
    "# Single example (noisy):\n",
    "transform₁ = (recursive_concept + sum_specific_details) - \n",
    "             (iterative_concept + sum_specific_details)\n",
    "           = recursive_concept - iterative_concept + noise₁\n",
    "\n",
    "# Multiple examples:\n",
    "avg_transform = mean([\n",
    "    recursive_concept - iterative_concept + noise₁,\n",
    "    recursive_concept - iterative_concept + noise₂,\n",
    "    recursive_concept - iterative_concept + noise₃\n",
    "])\n",
    "\n",
    "# Noise cancels out!\n",
    "avg_transform ≈ recursive_concept - iterative_concept\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc77a06",
   "metadata": {},
   "source": [
    "The multiple-exemplar approach teaches us that the standard lessons from machine learning apply here: one example is anecdote, and many examples are data.\n",
    "\n",
    "This principle extends beyond code embeddings to:\n",
    "- Style transfer in images (multiple examples of \"Picasso style\")\n",
    "- Voice conversion (multiple examples of speaker A → speaker B)\n",
    "- Language translation (multiple phrase pairs)\n",
    "\n",
    "The beauty of this is that the embedding space has **linear structure** that lets us manipulate concepts algebraically! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa7dfce",
   "metadata": {},
   "source": [
    "## The Theory Behind Code Embeddings\n",
    "\n",
    "Why do code embeddings work so well? The secret lies in how these models are trained.\n",
    "\n",
    "UniXcoder and similar models use several training objectives:\n",
    "\n",
    "1. **Masked Language Modeling (MLM)**: Randomly mask tokens and predict them\n",
    "   ```python\n",
    "   # Input:  \"def sort([MASK]): return sorted(arr)\"\n",
    "   # Predict: \"arr\"\n",
    "   ```\n",
    "\n",
    "2. **Contrastive Learning**: Similar code should have similar embeddings\n",
    "   - Positive pairs: (code, docstring), (original, paraphrased)\n",
    "   - Negative pairs: Random unrelated code\n",
    "\n",
    "3. **Next Token Prediction**: Predict the next token in a sequence\n",
    "\n",
    "The ideal code embedding model should capture semantic, be robust to variable naming and style and generalize to unseen code patterns. The increase in the performance and efficiency of modern code models represents a profound shift in how machines can reason about computation itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b98d1cb",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "UniXcoder isn't perfect. It has a maximum context length (512 tokens), so very long functions get truncated. It was trained primarily on popular languages (Python, Java, JavaScript), so it might not understand exotic languages as well. And it captures syntactic and structural similarities better than deep semantic equivalence—two functions that compute the same result through completely different algorithms might not score as similar as you'd expect.\n",
    "\n",
    "Most importantly, embeddings compress complex code into fixed-size vectors. Information is inevitably lost. Two functions might have the same embedding despite subtle but important differences. The model provides similarity scores, not guarantees of equivalence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "programming-genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
