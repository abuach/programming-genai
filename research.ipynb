{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb05cc3a",
   "metadata": {},
   "source": [
    "# Research\n",
    "\n",
    "AI and large language models represent a convergence of decades of research in machine learning, natural language processing, computer systems, and human-computer interaction. This chapter highlights foundational papers, influential systems, and active research areas for readers who want to explore the topics we've covered in greater depth.\n",
    "\n",
    "## Foundations of Language Models\n",
    "\n",
    "- The transformer architecture that started it all:\n",
    "  - Vaswani et al. (2017): *Attention Is All You Need* {cite}`vaswani2017attention`\n",
    "  - Devlin et al. (2019): *BERT: Pre-training of Deep Bidirectional Transformers* {cite}`devlin2019bert`\n",
    "\n",
    "- Scaling laws and emergent capabilities:\n",
    "  - Kaplan et al. (2020): *Scaling Laws for Neural Language Models* {cite}`kaplan2020scaling`\n",
    "  - Wei et al. (2022): *Emergent Abilities of Large Language Models* {cite}`wei2022emergent`\n",
    "\n",
    "## Prompting and In-Context Learning\n",
    "\n",
    "- Understanding how LLMs learn from prompts:\n",
    "  - Brown et al. (2020): *Language Models are Few-Shot Learners* {cite}`brown2020language`\n",
    "  - Wei et al. (2022): *Chain-of-Thought Prompting Elicits Reasoning* {cite}`wei2022chain`\n",
    "  - Kojima et al. (2022): *Large Language Models are Zero-Shot Reasoners* {cite}`kojima2022large`\n",
    "\n",
    "- Prompt engineering techniques:\n",
    "  - Reynolds and McDonell (2021): *Prompt Programming for Large Language Models* {cite}`reynolds2021prompt`\n",
    "  - Zhou et al. (2023): *Large Language Models Are Human-Level Prompt Engineers* {cite}`zhou2023large`\n",
    "\n",
    "\n",
    "## Tokenization and Representation\n",
    "\n",
    "- Subword tokenization methods:\n",
    "  - Sennrich et al. (2016): *Neural Machine Translation of Rare Words with Subword Units* {cite}`sennrich2016neural`\n",
    "  - Kudo and Richardson (2018): *SentencePiece: A Simple and Language Independent Approach* {cite}`kudo2018sentencepiece`\n",
    "\n",
    "- Understanding embeddings:\n",
    "  - Mikolov et al. (2013): *Efficient Estimation of Word Representations in Vector Space* {cite}`mikolov2013efficient`\n",
    "  - Pennington et al. (2014): *GloVe: Global Vectors for Word Representation* {cite}`pennington2014glove`\n",
    "\n",
    "## Alignment and Safety\n",
    "\n",
    "- Reinforcement learning from human feedback:\n",
    "  - Christiano et al. (2017): *Deep Reinforcement Learning from Human Preferences* {cite}`christiano2017deep`\n",
    "  - Ouyang et al. (2022): *Training Language Models to Follow Instructions with Human Feedback* {cite}`ouyang2022training`\n",
    "\n",
    "- Understanding model behavior and safety:\n",
    "  - Bai et al. (2022): *Constitutional AI: Harmlessness from AI Feedback* {cite}`bai2022constitutional`\n",
    "  - Ganguli et al. (2023): *The Capacity for Moral Self-Correction in Large Language Models* {cite}`ganguli2023capacity`\n",
    "\n",
    "- Anthropic alignment research:\n",
    "  - Bai et al. (2022): *Constitutional AI: Harmlessness from AI Feedback* {cite}`bai2022constitutional`\n",
    "    — Introduces Constitutional AI, a training framework where an AI uses a set of guiding principles (a “constitution”) to self-evaluate and improve harmlessness and helpfulness without requiring extensive human labeling.   \n",
    "  - Greenblatt et al. (2024): *Alignment Faking in Large Language Models* {cite}`greenblatt2024alignmentfaking`\n",
    "    — Empirically investigates “alignment faking,” where models trained for harmless behavior can strategically produce harmful content during training to preserve internal preferences. \n",
    "  - Sheshadri et al. (2025): *Why Do Some Language Models Fake Alignment While Others Don’t?* {cite}`sheshadri2025why`\n",
    "    — Analyzes differences in alignment-faking behaviors across models and explores factors influencing compliance gaps. {cite}`sheshadri2025why`\n",
    "  - MacDiarmid et al. (2025): *Natural Emergent Misalignment from Reward Hacking in Production RL* {cite}`macdiarmid2025natural`\n",
    "    — Shows that reward hacking in RL environments can lead to emergent misaligned behaviors including alignment faking and sabotage, and studies mitigations like inoculation prompting. \n",
    "  - Lynch et al. (2025): *Agentic Misalignment: How LLMs Could Be Insider Threats* {cite}`lynch2025agentic`\n",
    "    — Lyncg stress-tests 16 leading agentic models in simulated corporate environments and finds that, when faced with threats to their continued operation or conflicts between their goals and company direction, models sometimes engage in harmful insider-threat-like behaviors (e.g., blackmail, leaking sensitive information), highlighting risks from agentic misalignment and the need for more robust oversight. \n",
    "\n",
    "\n",
    "## Privacy and Security\n",
    "\n",
    "- Privacy-preserving machine learning:\n",
    "  - Abadi et al. (2016): *Deep Learning with Differential Privacy* {cite}`abadi2016deep`\n",
    "  - McMahan et al. (2017): *Communication-Efficient Learning of Deep Networks from Decentralized Data* {cite}`mcmahan2017communication`\n",
    "\n",
    "- Adversarial robustness and attacks:\n",
    "  - Carlini et al. (2021): *Extracting Training Data from Large Language Models* {cite}`carlini2021extracting`\n",
    "  - Wallace et al. (2019): *Universal Adversarial Triggers for Attacking and Analyzing NLP* {cite}`wallace2019universal`\n",
    "  - Souly et al. (2025): Poisoning Attacks on LLMs Require a Near-Constant Number of Poison Samples {cite}`souly2025poisoning`\n",
    "\n",
    "\n",
    "## Multimodal Models\n",
    "\n",
    "- Vision-language models:\n",
    "  - Radford et al. (2021): *Learning Transferable Visual Models From Natural Language Supervision* {cite}`radford2021learning`\n",
    "  - Ramesh et al. (2022): *Hierarchical Text-Conditional Image Generation with CLIP Latents* {cite}`ramesh2022hierarchical`\n",
    "\n",
    "- Unified architectures:\n",
    "  - Alayrac et al. (2022): *Flamingo: a Visual Language Model for Few-Shot Learning* {cite}`alayrac2022flamingo`\n",
    "\n",
    "## Model Efficiency and Compression\n",
    "\n",
    "- Making models faster and smaller:\n",
    "  - Hinton et al. (2015): *Distilling the Knowledge in a Neural Network* {cite}`hinton2015distilling`\n",
    "  - Frantar and Alistarh (2023): *SparseGPT: Massive Language Models Can Be Accurately Pruned* {cite}`frantar2023sparsegpt`\n",
    "\n",
    "- Low-rank adaptation:\n",
    "  - Hu et al. (2022): *LoRA: Low-Rank Adaptation of Large Language Models* {cite}`hu2022lora`\n",
    "\n",
    "## Retrieval Augmentation and Tool Use\n",
    "\n",
    "- Enhancing models with external knowledge:\n",
    "  - Lewis et al. (2020): *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks* {cite}`lewis2020retrieval`\n",
    "  - Borgeaud et al. (2022): *Improving Language Models by Retrieving from Trillions of Tokens* {cite}`borgeaud2022improving`\n",
    "\n",
    "- Tool use and agentic behavior:\n",
    "  - Schick et al. (2023): *Toolformer: Language Models Can Teach Themselves to Use Tools* {cite}`schick2023toolformer`\n",
    "  - Nakano et al. (2021): *WebGPT: Browser-assisted Question-Answering with Human Feedback* {cite}`nakano2021webgpt`\n",
    "\n",
    "\n",
    "## Interpretability and Mechanistic Understanding\n",
    "\n",
    "- Understanding how transformers work:\n",
    "  - Elhage et al. (2021): *A Mathematical Framework for Transformer Circuits* {cite}`elhage2021mathematical`\n",
    "  - Olsson et al. (2022): *In-Context Learning and Induction Heads* {cite}`olsson2022context`\n",
    "\n",
    "- Probing and analysis:\n",
    "  - Tenney et al. (2019): *BERT Rediscovers the Classical NLP Pipeline* {cite}`tenney2019bert`\n",
    "\n",
    "## Coding and Program Synthesis\n",
    "\n",
    "- Models for code generation:\n",
    "  - Chen et al. (2021): *Evaluating Large Language Models Trained on Code* {cite}`chen2021evaluating`\n",
    "  - Austin et al. (2021): *Program Synthesis with Large Language Models* {cite}`austin2021program`\n",
    "\n",
    "- Formal verification and correctness:\n",
    "  - Polu and Sutskever (2020): *Generative Language Modeling for Automated Theorem Proving* {cite}`polu2020generative`\n",
    "\n",
    "## Practical Resources and Libraries\n",
    "\n",
    "- Open-source frameworks:\n",
    "  - [Ollama](https://docs.ollama.com/)\n",
    "  - [UniXcoder](https://huggingface.co/microsoft/unixcoder-base)\n",
    "\n",
    "## Surveys and Perspectives\n",
    "\n",
    "- Comprehensive overviews:\n",
    "  - Zhao et al. (2023): *A Survey of Large Language Models* {cite}`zhao2023survey`\n",
    "  - Bommasani et al. (2021): *On the Opportunities and Risks of Foundation Models* {cite}`bommasani2021opportunities`\n",
    "\n",
    "- Future directions and open problems:\n",
    "  - Bubeck et al. (2023): *Sparks of Artificial General Intelligence: Early Experiments with GPT-4* {cite}`bubeck2023sparks`\n",
    "\n",
    "## References\n",
    "\n",
    "In the web/html version of the book, the bibliography will appear *directly below this current text section*.\n",
    "\n",
    "However in the print versions which are based on $\\LaTeX$, the bibliography will appear (more traditionally) as the penultimate un-numbered standalone chapter which precedes the Proof Index.\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrt\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
