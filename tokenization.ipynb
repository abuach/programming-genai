{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee89ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Configure your server URL here\n",
    "SERVER_HOST = 'http://ollama.cs.wallawalla.edu:11434'\n",
    "client = ollama.Client(host=SERVER_HOST)\n",
    "\n",
    "def call_ollama(prompt, model=\"cs450\", **options):\n",
    "    \"\"\"\n",
    "    Send a prompt to the Ollama API.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The prompt to send\n",
    "        model (str): Model name to use\n",
    "        **options: Additional model parameters (temperature, top_k, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        str: The model's response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            options=options\n",
    "        )\n",
    "        return response['response']\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def call_ollama_full(prompt, model=\"cs450\", **options):\n",
    "    try:\n",
    "        response = client.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            options=options\n",
    "        )\n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e525d6ed",
   "metadata": {},
   "source": [
    "# Tokenization and Code Representation\n",
    "\n",
    "```{admonition} Learning Objectives\n",
    "After reading this chapter, you will be able to:\n",
    "* **Define** tokenization and articulate its fundamental role as the critical \"bridge\" between human-readable source code and the numerical representations processed by Large Language Models (LLMs).1  \n",
    "* **Explain** why tokenization is a more complex challenge for source code than for natural language, citing specific code properties such as precise syntax, meaningful whitespace (e.g., Python indentation), case sensitivity, and special operators.  \n",
    "* **Describe** the conceptual mechanism of *Byte Pair Encoding (BPE)*, including its iterative, frequency-based merging process, and explain how it attempts to balance vocabulary size with representational efficiency.    \n",
    "* **Analyze** and predict how different code styles (e.g. long vs. short variable names) are tokenized and explain the resulting impact on token efficiency and context window consumption.  \n",
    "* **Connect** specific tokenization inefficiencies, such as the fragmentation of rare identifiers, to concrete negative outcomes, including poor code generation quality, loss of structural information, and increased API or operational costs.  \n",
    "* **Evaluate** the function of *special tokens* used in code-specific models, including structural markers (\\<|endoftext|\\>), fill-in-the-middle (FIM) tokens (\\<|fim\\_prefix|\\>), and language-specific markers (\\<|python|\\>).  \n",
    "* **Apply** the principle of \"token budget management\" to author more concise and efficient prompts, thereby maximizing the available *context window* for model responses and retrieved context.  \n",
    "* **Debug** certain unexpected LLM behaviors or generation failures by hypothesizing a root cause in the tokenization process.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b33dcb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Before a language model can process code, it must first convert that code into a representation it can understand. This conversion process — **tokenization** — is more fundamental than it might initially appear. The way code is tokenized directly impacts what the model \"sees,\" what patterns it can learn, and ultimately how well it can generate, understand, and manipulate code.\n",
    "\n",
    "Unlike natural language, where tokenization boundaries often align with word boundaries, code presents unique challenges. Should `getUserById` be treated as one token or split into `get`, `User`, `By`, `Id`? How should operators like `->`, `==`, or `>>>` be handled? What about indentation in Python, where whitespace carries semantic meaning?\n",
    "\n",
    "This chapter explores how language models represent code at the token level, why these representations matter for code generation tasks, and how understanding tokenization helps you write better prompts and debug model behavior.\n",
    "\n",
    "## The Basics: What is Tokenization?\n",
    "\n",
    "Tokenization is the process of breaking text (including code) into discrete units called **tokens**. These tokens become the atomic elements the model operates on — it reads tokens, thinks in tokens, and generates tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17bb4e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding Tokenization\n",
      "============================================================\n",
      "\n",
      "Text: 'hello'\n",
      "Response: 3 tokens\n",
      "\n",
      "Explanation: The word \"hello\" is usually represented by 3 tokens in most language models, including the start-of-word token, the word itself, and the end-of-word token.\n",
      "\n",
      "Text: 'hello world'\n",
      "Response: 5 tokens\n",
      "\n",
      "Explanation: The text \"hello world\" is typically represented using 5 tokens in most language models, including the space between \"hello\" and \"world\".\n",
      "\n",
      "Text: 'getUserById'\n",
      "Response: 3 tokens\n",
      "\n",
      "Explanation: The phrase \"getUserById\" is a simple function name or method call, which can be represented by three tokens in most language models: one for the verb (\"get\"), one for the noun (\"User\"), and one for the identifier (\"byId\").\n",
      "\n",
      "Text: 'get_user_by_id'\n",
      "Response: 3\n",
      "\n",
      "The term \"get_user_by_id\" is a simple function name consisting of three words, which would typically be represented by three tokens in most language models.\n",
      "\n",
      "Text: 'x = y + z'\n",
      "Response: 3\n",
      "\n",
      "The text \"x = y + z\" is represented by three tokens in most language models: one for each variable (x, y, z) and one for the operator (+).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_tokenization_concept():\n",
    "    \"\"\"Show how models see text as tokens, not characters.\"\"\"\n",
    "    \n",
    "    # Ask the model to count tokens in different strings\n",
    "    test_strings = [\n",
    "        \"hello\",\n",
    "        \"hello world\",\n",
    "        \"getUserById\",\n",
    "        \"get_user_by_id\",\n",
    "        \"x = y + z\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Understanding Tokenization\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    for string in test_strings:\n",
    "        prompt = f\"\"\"How many tokens would a language model typically use to represent this text: \"{string}\"\n",
    "        \n",
    "Just give me a number and brief explanation.\"\"\"\n",
    "        \n",
    "        response = call_ollama(prompt, temperature=0.2, num_predict=60)\n",
    "        print(f\"Text: '{string}'\")\n",
    "        print(f\"Response: {response}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_tokenization_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c1253b",
   "metadata": {},
   "source": [
    "```{note}\n",
    "\n",
    "**Key insight**: A token is not always a word or character. Modern tokenizers use **subword tokenization**, where common sequences (like \"def\" in Python or \"function\" in JavaScript) might be single tokens, while rare words might be split into multiple tokens.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e786133d",
   "metadata": {},
   "source": [
    "```{note}\n",
    "\n",
    "> Token Representation: From Text to Numbers\n",
    "\n",
    "At the lowest level, models don't work with text at all — they work with numbers. Each token is assigned a unique integer ID from a vocabulary.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf098cb",
   "metadata": {},
   "source": [
    "## Code Tokenization \n",
    "\n",
    "Let's explore how code elements are typically tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7275c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Tokenization Patterns\n",
      "============================================================\n",
      "\n",
      "Code: def calculate_sum(a, b):\n",
      "Tokenization: def|calculate_sum|(|a|,|b||)|:\n",
      "\n",
      "Code: function calculateSum(a, b) {\n",
      "Tokenization: function|calculateSum|(|a|,|b||)|{\n",
      "\n",
      "Code: public static void main(String[] args) {\n",
      "Tokenization: public|static|void|main|(String[]|args)|{|}|\n",
      "\n",
      "Code: x = [i**2 for i in range(10)]\n",
      "Tokenization: x | = | [ | i | ** | 2 | for | i | in | range | ( | 10 | ) | ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def explore_vocabulary():\n",
    "    \"\"\"Explore how different code elements might be tokenized.\"\"\"\n",
    "    \n",
    "    code_samples = [\n",
    "        \"def calculate_sum(a, b):\",\n",
    "        \"function calculateSum(a, b) {\",\n",
    "        \"public static void main(String[] args) {\",\n",
    "        \"x = [i**2 for i in range(10)]\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Code Tokenization Patterns\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    for code in code_samples:\n",
    "        prompt = f\"\"\"For a code-specialized language model, describe how this code would likely be tokenized:\n",
    "\n",
    "Code: {code}\n",
    "\n",
    "List the approximate tokens (split by '|' at the likely token boundaries). Be brief.\"\"\"\n",
    "        \n",
    "        response = call_ollama(prompt, temperature=0.1, num_predict=120)\n",
    "        print(f\"Code: {code}\")\n",
    "        print(f\"Tokenization: {response}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    explore_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ac1e1c",
   "metadata": {},
   "source": [
    "```{note}\n",
    "\n",
    "**Typical Vocabulary Size**: Modern LLMs have vocabularies of 32,000 to 100,000+ tokens. Code-specialized models often have larger vocabularies to efficiently represent programming constructs.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcd6bec",
   "metadata": {},
   "source": [
    "## Why Tokenization Matters for Code\n",
    "\n",
    "Unlike natural language, code has:\n",
    "- **Precise syntax**: Every character can matter (`=` vs `==`)\n",
    "- **Meaningful whitespace**: Indentation in Python, formatting in all languages\n",
    "- **Special operators**: `->`, `::`, `>>>`, `**`, etc.\n",
    "- **Case sensitivity**: `userName` vs `UserName` vs `USERNAME`\n",
    "- **Domain-specific identifiers**: API names, library functions, variable names\n",
    "\n",
    "Poor tokenization can lead to:\n",
    "- Inefficient representation (more tokens = less context fits in the window)\n",
    "- Loss of structural information\n",
    "- Difficulty learning patterns\n",
    "- Poor generation quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ba85060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization Impact on Understanding\n",
      "============================================================\n",
      "\n",
      "Code:\n",
      "def get_user_by_id(user_id):\n",
      "    return database.find(user_id)\n",
      "\n",
      "Assessment: Yes, the provided code snippet follows a simple and straightforward style that is generally considered acceptable in Python for defining a function to retrieve a user by their ID from a database. The function name `get_user_by_id` clearly describes its purpose, and the\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Code:\n",
      "def getUserById(userId):\n",
      "    return database.find(userId)\n",
      "\n",
      "Assessment: Yes, the provided code snippet is in proper Python code style. It follows the PEP 8 guidelines for function naming and uses clear, concise syntax. The function `getUserById` takes a parameter `userId` and returns the result of calling the\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Code:\n",
      "def GETUSERBYID(USERID):\n",
      "    return DATABASE.FIND(USERID)\n",
      "\n",
      "Assessment: No, this is not proper code style. The function name should be in lowercase with words separated by underscores, and it's a good practice to include type hints for parameters and return values. Here's an improved version:\n",
      "\n",
      "```python\n",
      "def get_user\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_tokenization_impact():\n",
    "    \"\"\"Show how tokenization affects model understanding.\"\"\"\n",
    "    \n",
    "    # Same functionality, different naming conventions\n",
    "    code_variants = [\n",
    "        \"def get_user_by_id(user_id):\\n    return database.find(user_id)\",\n",
    "        \"def getUserById(userId):\\n    return database.find(userId)\",\n",
    "        \"def GETUSERBYID(USERID):\\n    return DATABASE.FIND(USERID)\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Tokenization Impact on Understanding\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    for code in code_variants:\n",
    "        prompt = f\"\"\"Is this proper code style? One sentence.\n",
    "\n",
    "{code}\n",
    "\n",
    "Assessment:\"\"\"\n",
    "        \n",
    "        response = call_ollama(prompt, temperature=0.3, num_predict=50)\n",
    "        print(f\"Code:\\n{code}\\n\")\n",
    "        print(f\"Assessment: {response}\\n\")\n",
    "        print(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_tokenization_impact()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476d19ef",
   "metadata": {},
   "source": [
    "```{note}\n",
    "\n",
    "The model's response will vary based on how the identifiers are tokenized. CamelCase and snake_case are typically handled well, but ALL_CAPS might fragment differently.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c00f06",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding (BPE): The Standard Approach\n",
    "\n",
    "Most modern LLMs use **Byte Pair Encoding (BPE)** or variants like **WordPiece** or **SentencePiece**. BPE learns a vocabulary by iteratively merging the most frequent character pairs.\n",
    "\n",
    "### How BPE Works (Simplified)\n",
    "\n",
    "1. Start with character-level vocabulary\n",
    "2. Find most frequent adjacent pair\n",
    "3. Merge this pair into a new token\n",
    "4. Repeat until vocabulary reaches desired size\n",
    "\n",
    "In the following example, we: \n",
    "\n",
    "- Use the `tiktoken` library to actually count tokens\n",
    "- Show common/rare tokenization differences (e.g., \"function\" = 1 token, \"funcxzqtion\" = 3-4 tokens)\n",
    "\n",
    "`tiktoken` is a fast Byte Pair Encoding (BPE) tokenizer developed by OpenAI for use with their language models. It allows you to convert text into tokens (numerical representations) and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0cc3075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Token Counts: Common vs Rare\n",
      "==================================================\n",
      "\n",
      "Common word     | function                  | 1 tokens\n",
      "Rare word       | funcxzqtion               | 4 tokens\n",
      "Common code     | def factorial(n):         | 4 tokens\n",
      "Rare code       | def qzxfactorial(n):      | 8 tokens\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens using tiktoken (approximates most BPE tokenizers).\"\"\"\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")  # GPT-4 encoding\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "def explore_bpe_behavior():\n",
    "    \"\"\"Show how common vs rare patterns use different token counts.\"\"\"\n",
    "    \n",
    "    examples = [\n",
    "        (\"Common word\", \"function\"),\n",
    "        (\"Rare word\", \"funcxzqtion\"),\n",
    "        (\"Common code\", \"def factorial(n):\"),\n",
    "        (\"Rare code\", \"def qzxfactorial(n):\"),\n",
    "    ]\n",
    "    \n",
    "    print(\"BPE Token Counts: Common vs Rare\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    for label, text in examples:\n",
    "        tokens = count_tokens(text)\n",
    "        print(f\"{label:15} | {text:25} | {tokens} tokens\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    explore_bpe_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f47654d",
   "metadata": {},
   "source": [
    "Alternatively, we could use `prompt_eval_count` from the model's response to get the actual token counts, showing that common patterns like \"function\" tokenize more efficiently than rare variants like \"funcxzqtion\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d373b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Token Counts: Common vs Rare\n",
      "==================================================\n",
      "\n",
      "Common word     | function                  | 30 tokens\n",
      "Rare word       | funcxzqtion               | 33 tokens\n",
      "Common code     | def factorial(n):         | 33 tokens\n",
      "Rare code       | def qzxfactorial(n):      | 37 tokens\n"
     ]
    }
   ],
   "source": [
    "def count_tokens(text):\n",
    "    \"\"\"Get actual token count from Ollama.\"\"\"\n",
    "    response = call_ollama_full(\n",
    "        text,\n",
    "        temperature=0,\n",
    "        num_predict=1,\n",
    "        format=\"json\"  # Returns full JSON response instead of just text\n",
    "    )\n",
    "    return response['prompt_eval_count']\n",
    "\n",
    "def explore_bpe_behavior():\n",
    "    \"\"\"Show how common vs rare patterns use different token counts.\"\"\"\n",
    "    \n",
    "    examples = [\n",
    "        (\"Common word\", \"function\"),\n",
    "        (\"Rare word\", \"funcxzqtion\"),\n",
    "        (\"Common code\", \"def factorial(n):\"),\n",
    "        (\"Rare code\", \"def qzxfactorial(n):\"),\n",
    "    ]\n",
    "    \n",
    "    print(\"BPE Token Counts: Common vs Rare\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    for label, text in examples:\n",
    "        tokens = count_tokens(text)\n",
    "        print(f\"{label:15} | {text:25} | {tokens} tokens\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    explore_bpe_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8ae6a",
   "metadata": {},
   "source": [
    "```{note}\n",
    "\n",
    "**Key principle**: Common sequences become single tokens (efficient), rare sequences fragment into multiple tokens (less efficient).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f686038f",
   "metadata": {},
   "source": [
    "## Special Tokens and Code Structure\n",
    "\n",
    "Code models use special tokens to mark structural boundaries:\n",
    "\n",
    "- `<|endoftext|>` - End of document\n",
    "- `<|fim_prefix|>`, `<|fim_suffix|>`, `<|fim_middle|>` - Fill-in-the-middle tasks\n",
    "- `<|python|>`, `<|javascript|>` - Language markers\n",
    "- `\\n`, `\\t` - Whitespace (often separate tokens)\n",
    "\n",
    "This is crucial to improve the quality of code completion tasks based on the current context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f78f722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-level (module scope):\n",
      "  → ```python\n",
      "result = calculate()\n",
      "```\n",
      "\n",
      "Inside function body:\n",
      "  → ```python\n",
      "def process(x):\n",
      "    result = calculate(x)\n",
      "```\n",
      "\n",
      "After if statement:\n",
      "  → ```python\n",
      "result = calculate(data) if data else None\n",
      "```\n",
      "\n",
      "In class method:\n",
      "  → ```python\n",
      "result = calculate(self.data)\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_boundary_effects():\n",
    "    \"\"\"Demonstrate how structural context changes completion.\"\"\"\n",
    "    \n",
    "    # Same incomplete code in different structural positions\n",
    "    incomplete = \"result = calculate\"\n",
    "    \n",
    "    scenarios = [\n",
    "        (\"Top-level (module scope)\", \n",
    "         f\"{incomplete}\"),\n",
    "        \n",
    "        (\"Inside function body\",\n",
    "         f\"def process(x):\\n    {incomplete}\"),\n",
    "        \n",
    "        (\"After if statement\",\n",
    "         f\"if data:\\n    {incomplete}\"),\n",
    "        \n",
    "        (\"In class method\",\n",
    "         f\"class Processor:\\n def __init__(self, x): self.data = x \\n def run(self):\\n    {incomplete}\")\n",
    "    ]\n",
    "    \n",
    "    for label, code in scenarios:\n",
    "        response = call_ollama(\n",
    "            f\"\"\"Assume `calculate is a simple arithmetic operation for demonstration. \n",
    "                Complete this Python code with only one line:\\n\\n{code}\"\"\",\n",
    "            temperature=0.1,\n",
    "            num_predict=31\n",
    "        )\n",
    "        print(f\"{label}:\")\n",
    "        print(f\"  → {response}\\n\")\n",
    "\n",
    "show_boundary_effects()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a563f9",
   "metadata": {},
   "source": [
    "**Example output patterns shown:**\n",
    "- Top-level: (function call)\n",
    "- Inside function: (with parameter)\n",
    "- After if: (action-oriented)\n",
    "- In class method: (uses `self`)\n",
    "\n",
    "The model learns that different structural positions (module vs. function vs. class) have different token distribution patterns, leading to contextually appropriate completions.\n",
    "\n",
    "In the above example, we use one concrete example (the incomplete statement) across various contexts, demonstrating the key insight: position and context really matter!\n",
    "\n",
    "The model's completion changes based on structural context because different token patterns are statistically associated with different code structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef6dedd",
   "metadata": {},
   "source": [
    "## Indentation and Whitespace Tokenization\n",
    "\n",
    "Python's significant whitespace poses unique challenges. Models must learn that indentation carries semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bbb4e102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indentation Understanding\n",
      "============================================================\n",
      "\n",
      "Correct indentation\n",
      "Code:\n",
      "def greet(name):\n",
      "    print(f\"Hello, {name}\")\n",
      "    return name\n",
      "\n",
      "Model says: Yes.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Incorrect indentation\n",
      "Code:\n",
      "def greet(name):\n",
      "print(f\"Hello, {name}\")\n",
      "    return name\n",
      "\n",
      "Model says: No. The `print` statement is not indented correctly. In Python, indentation is crucial for defining the blocks of code. Here's the corrected version:\n",
      "\n",
      "```python\n",
      "def greet(name):\n",
      "    print(f\"Hello, {name}\")\n",
      "    return name\n",
      "```\n",
      "\n",
      "Now it should work as expected.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Missing indentation\n",
      "Code:\n",
      "def greet(name):\n",
      "print(f\"Hello, {name}\")\n",
      "return name\n",
      "\n",
      "Model says: No. The `print` statement is not indented correctly. In Python, indentation is crucial as it defines the blocks of code.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_indentation_understanding():\n",
    "    \"\"\"Test if model understands indentation semantics.\"\"\"\n",
    "    \n",
    "    code_samples = [\n",
    "        (\"Correct indentation\", \"\"\"def greet(name):\n",
    "    print(f\"Hello, {name}\")\n",
    "    return name\"\"\"),\n",
    "        (\"Incorrect indentation\", \"\"\"def greet(name):\n",
    "print(f\"Hello, {name}\")\n",
    "    return name\"\"\"),\n",
    "        (\"Missing indentation\", \"\"\"def greet(name):\n",
    "print(f\"Hello, {name}\")\n",
    "return name\"\"\")\n",
    "    ]\n",
    "    \n",
    "    print(\"Indentation Understanding\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    for label, code in code_samples:\n",
    "        prompt = f\"\"\"Is this Python code correct? Answer yes or no and explain very briefly.\n",
    "\n",
    "{code}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        response = call_ollama(prompt, temperature=0.1, num_predict=70)\n",
    "        print(f\"{label}\")\n",
    "        print(f\"Code:\\n{code}\\n\")\n",
    "        print(f\"Model says: {response}\\n\")\n",
    "        print(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_indentation_understanding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63519f",
   "metadata": {},
   "source": [
    "Code models learn to associate indentation tokens with control flow and scope, enabling them to generate properly indented code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1919a4",
   "metadata": {},
   "source": [
    "## Tokenization Efficiency: Token Count Matters\n",
    "\n",
    "The number of tokens affects:\n",
    "- **Context window usage**: Fewer tokens = more context fits\n",
    "- **Generation cost**: More tokens = higher API costs\n",
    "- **Processing speed**: More tokens = slower inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04b62497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Efficiency Comparison\n",
      "============================================================\n",
      "\n",
      "Verbose: 48 tokens\n",
      "Code:\n",
      "def calculate_sum_of_squares(input_numbers):\n",
      "    total_sum = 0\n",
      "    for individual_number in input_numbers:\n",
      "        squared_value = individual_number * individual_number\n",
      "        total_sum = total_sum + squared_value\n",
      "    return total_sum\n",
      "\n",
      "Tokens: [755, 11294, 10370, 3659, 646, 41956, 5498, 34064, 997, 262]...\n",
      "Efficiency: 4.9 chars/token\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Concise: 17 tokens\n",
      "Code:\n",
      "def sum_squares(nums):\n",
      "    return sum(n * n for n in nums)\n",
      "\n",
      "Tokens: [755, 2694, 646, 41956, 21777, 997, 262, 471, 2694, 1471]...\n",
      "Efficiency: 3.4 chars/token\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Mathematical: 17 tokens\n",
      "Code:\n",
      "def sum_squares(nums):\n",
      "    return sum(n**2 for n in nums)\n",
      "\n",
      "Tokens: [755, 2694, 646, 41956, 21777, 997, 262, 471, 2694, 1471]...\n",
      "Efficiency: 3.4 chars/token\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compare_token_efficiency():\n",
    "    \"\"\"Compare token efficiency of different coding styles.\"\"\"\n",
    "    \n",
    "    # Use GPT-4 tokenizer (cl100k_base) as representative example\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    implementations = [\n",
    "        (\"Verbose\", \"\"\"def calculate_sum_of_squares(input_numbers):\n",
    "    total_sum = 0\n",
    "    for individual_number in input_numbers:\n",
    "        squared_value = individual_number * individual_number\n",
    "        total_sum = total_sum + squared_value\n",
    "    return total_sum\"\"\"),\n",
    "        \n",
    "        (\"Concise\", \"\"\"def sum_squares(nums):\n",
    "    return sum(n * n for n in nums)\"\"\"),\n",
    "        \n",
    "        (\"Mathematical\", \"\"\"def sum_squares(nums):\n",
    "    return sum(n**2 for n in nums)\"\"\")\n",
    "    ]\n",
    "    \n",
    "    print(\"Token Efficiency Comparison\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    for label, code in implementations:\n",
    "        tokens = encoding.encode(code)\n",
    "        token_count = len(tokens)\n",
    "        \n",
    "        print(f\"{label}: {token_count} tokens\")\n",
    "        print(f\"Code:\\n{code}\\n\")\n",
    "        print(f\"Tokens: {tokens[:10]}...\" if len(tokens) > 10 else f\"Tokens: {tokens}\")\n",
    "        print(f\"Efficiency: {len(code) / token_count:.1f} chars/token\\n\")\n",
    "        print(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compare_token_efficiency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3d62c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "```{note}\n",
    "\n",
    "**Trade-off**: Verbose code may be more readable but consumes more tokens. Concise code is token-efficient but may be less clear.\n",
    "\n",
    "```\n",
    "\n",
    "```{note}\n",
    "\n",
    "Key insight: The verbose version uses **3.5x more tokens** for the same functionality, consuming more context window and costing more to process.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5a5cf9",
   "metadata": {},
   "source": [
    "## Cross-Language Tokenization\n",
    "\n",
    "Code models are typically trained on multiple languages. The tokenizer must handle diverse syntax efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6fc99e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Language Tokenization\n",
      "============================================================\n",
      "\n",
      "Python\n",
      "Code:\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "\n",
      "Analysis: In the given Python code:\n",
      "\n",
      "- `def`, `add`, `(`, `a`, `,`, `b`, `)`, `:` are single tokens.\n",
      "- `return`, `a`, `+`, `b` are also single tokens.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "JavaScript\n",
      "Code:\n",
      "function add(a, b) {\n",
      "    return a + b;\n",
      "}\n",
      "\n",
      "Analysis: - `function`, `add`, `(`, `a`, `,`, `b`, `)`, `{`, `return`, `a`, `+`, `b`, `;`, `}` would be tokenized as single tokens.\n",
      "- The keywords and symbols are typically treated as individual tokens in JavaScript parsing.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Java\n",
      "Code:\n",
      "public int add(int a, int b) {\n",
      "    return a + b;\n",
      "}\n",
      "\n",
      "Analysis: ```java\n",
      "public int add(int a, int b) { // Single token: public, int, add, (, int, a, , int, b, ), {, return, a, +, b, ;, }\n",
      "    return a + b; // Single token: return, a, +, b, ;\n",
      "}\n",
      "```\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Rust\n",
      "Code:\n",
      "fn add(a: i32, b: i32) -> i32 {\n",
      "    a + b\n",
      "}\n",
      "\n",
      "Analysis: ```rust\n",
      "fn, add, (, a, :, i32, ,, b, :, i32, ), ->, i32, {, a, +, b, }\n",
      "```\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def explore_cross_language_tokenization():\n",
    "    \"\"\"Explore how different languages tokenize.\"\"\"\n",
    "    \n",
    "    equivalent_code = [\n",
    "        (\"Python\", \"def add(a, b):\\n    return a + b\"),\n",
    "        (\"JavaScript\", \"function add(a, b) {\\n    return a + b;\\n}\"),\n",
    "        (\"Java\", \"public int add(int a, int b) {\\n    return a + b;\\n}\"),\n",
    "        (\"Rust\", \"fn add(a: i32, b: i32) -> i32 {\\n    a + b\\n}\")\n",
    "    ]\n",
    "    \n",
    "    print(\"Cross-Language Tokenization\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    for lang, code in equivalent_code:\n",
    "        prompt = f\"\"\"Which parts of this {lang} code would likely be tokenized as single tokens\n",
    "                        vs split into multiple tokens? Answer as briefly (in as few lines) as possible\n",
    "\n",
    "{code}\n",
    "\n",
    "Brief analysis:\"\"\"\n",
    "        \n",
    "        response = call_ollama(prompt, temperature=0.2, num_predict=200)\n",
    "        print(f\"{lang}\")\n",
    "        print(f\"Code:\\n{code}\\n\")\n",
    "        print(f\"Analysis: {response}\\n\")\n",
    "        print(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    explore_cross_language_tokenization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811146cb",
   "metadata": {},
   "source": [
    "```{note}\n",
    "\n",
    "**Observation**: Common keywords across languages (like \"function\", \"return\", \"int\", and \"add\") are likely single tokens, while language-specific syntax varies in efficiency.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f0988d",
   "metadata": {},
   "source": [
    "## Impact on Code Generation Quality\n",
    "\n",
    "Understanding tokenization helps explain common generation issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f357df15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Naming and Tokenization\n",
      "============================================================\n",
      "\n",
      "Prompt: Generate a Python function with a variable name for storing user authentication tokens\n",
      "Generated:\n",
      "```python\n",
      "auth_token = \"your_auth_token_here\"\n",
      "```\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt: Generate a Python function with a variable name for user auth tokens (use common abbreviation)\n",
      "Generated:\n",
      "```python\n",
      "auth_token = \"your_auth_token_here\"\n",
      "```\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Issue 1: Variable Name Fragments\n",
    "def demonstrate_naming_issues():\n",
    "    \"\"\"Show how tokenization affects variable naming.\"\"\"\n",
    "    \n",
    "    prompts = [\n",
    "        \"Generate a Python function with a variable name for storing user authentication tokens\",\n",
    "        \"Generate a Python function with a variable name for user auth tokens (use common abbreviation)\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Variable Naming and Tokenization\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        full_prompt = f\"{prompt}. Just show the line with the variable\"\n",
    "        \n",
    "        response = call_ollama(full_prompt, temperature=0.5, num_predict=60)\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Generated:\\n{response}\\n\")\n",
    "        print(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_naming_issues()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892af3d8",
   "metadata": {},
   "source": [
    "Uncommon or very long variable names may fragment into many tokens, leading models to avoid them or truncate them.\n",
    "\n",
    "Most likely, your coding model will abbreviate \"authentication\" to \"auth\" in *both* cases, regardless of instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b04dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator Spacing Consistency\n",
      "============================================================\n",
      "\n",
      "Sure, here are three variations of the Python expression `x = a + b * c` with different spacing:\n",
      "\n",
      "1. ```python\n",
      "x = a + (b * c)\n",
      "```\n",
      "\n",
      "2. ```python\n",
      "x= a+ b*c\n",
      "```\n",
      "\n",
      "3. ```python\n",
      "x =   a +    b  *  c\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "### Issue 2: Operator Spacing\n",
    "def demonstrate_spacing_consistency():\n",
    "    \"\"\"Show how models handle operator spacing.\"\"\"\n",
    "    \n",
    "    prompt = \"\"\"Generate 3 variations of this Python expression with different spacing:\n",
    "x = a + b * c\n",
    "\n",
    "Variations:\"\"\"\n",
    "    \n",
    "    print(\"Operator Spacing Consistency\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    response = call_ollama(prompt, temperature=0.7, num_predict=100)\n",
    "    print(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_spacing_consistency()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63151f9a",
   "metadata": {},
   "source": [
    "Models learn spacing patterns from training data. Inconsistent tokenization of operators can lead to inconsistent spacing in generated code.\n",
    "\n",
    "One good way to deal with this sort of issue is to ensure consistency by using prompting techniques which provide examples, as we'll see later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2150ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment Generation\n",
      "============================================================\n",
      "\n",
      "Certainly! Below is a Python function that calculates the factorial of a given number along with detailed comments explaining each part of the code:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    \"\"\"\n",
      "    Calculates the factorial of a non-negative integer n.\n",
      "    \n",
      "    Args:\n",
      "        n (int): A non-negative integer whose factorial is to be calculated.\n",
      "        \n",
      "    Returns:\n",
      "        int: The factorial of the input number n.\n",
      "        \n",
      "    Raises:\n",
      "        ValueError: If the input n is negative, as factorial is not defined for negative numbers.\n",
      "    \"\"\"\n",
      "    # Check if the input is a non-negative integer\n",
      "    if not isinstance(n, int) or n < 0:\n",
      "        raise ValueError(\"Input must be a non-negative integer.\")\n",
      "    \n",
      "    # Base case: factorial of 0 or 1 is 1\n",
      "    if n == 0 or n == 1:\n",
      "        return 1\n",
      "    \n",
      "    # Initialize the result to 1 (factorial starts from 1)\n",
      "    result = 1\n",
      "    \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "Without comments:\n",
      "```python\n",
      "def factorial(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n-1)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "### Issue 3: Comment Handling\n",
    "def explore_comment_tokenization():\n",
    "    \"\"\"Explore how comments are tokenized and generated.\"\"\"\n",
    "    \n",
    "    prompt = \"\"\"Write a Python function to calculate factorial with detailed comments:\"\"\"\n",
    "    \n",
    "    print(\"Comment Generation\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    response = call_ollama(prompt, temperature=0.5, num_predict=200)\n",
    "    print(response)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Now ask for code without comments\n",
    "    prompt2 = \"\"\"Write a Python function to calculate factorial with NO comments:\"\"\"\n",
    "    \n",
    "    response2 = call_ollama(prompt2, temperature=0.5, num_predict=150)\n",
    "    print(\"\\nWithout comments:\")\n",
    "    print(response2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    explore_comment_tokenization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a1b35e",
   "metadata": {},
   "source": [
    "Comments are tokenized just like code. Models must learn when to generate comments vs code based on token patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae6faa",
   "metadata": {},
   "source": [
    "```{note}\n",
    "\n",
    "The context window is measured in tokens. Understanding this can help with optimizing prompts. More concise prompts leave more room in the context window for the model's response and any retrieved context (in RAG systems).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ab7ef",
   "metadata": {},
   "source": [
    "Tokenization is the bridge between human-readable code and model-processable representations. By understanding this bridge, you can:\n",
    "- Write more effective prompts\n",
    "- Debug unexpected model behavior\n",
    "- Optimize context window usage\n",
    "- Predict when models will struggle (rare patterns, uncommon languages)\n",
    "- Design better coding conventions for AI-assisted development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140483d9",
   "metadata": {},
   "source": [
    "## **Summary**\n",
    "\n",
    "This chapter establishes tokenization as a fundamental, performance-critical process that dictates how a Large Language Model \"sees,\" \"thinks about,\" and \"generates\" source code. It is not a trivial preprocessing step but rather the essential bridge connecting the symbolic, human-readable world of programming to the numerical, model-processable world of token IDs.\n",
    "\n",
    "The chapter explains that modern LLMs for code, like their natural language counterparts, overwhelmingly rely on *subword tokenization* algorithms. The most prominent of these, *Byte Pair Encoding (BPE)*, is explored. The BPE process is conceptually straightforward: it begins with a base vocabulary of individual characters and iteratively merges the most frequently occurring adjacent pair of tokens in its training data into a new, single token. This process is repeated until a predefined vocabulary size (e.g., 50,000 or 100,000+ tokens) is reached. The result is a highly efficient vocabulary where common sequences—such as programming keywords (def, import, function) or standard library names—are represented as single, efficient tokens. Conversely, rare or novel sequences—like uncommon variable names or project-specific API identifiers—are fragmented into multiple, less efficient subword tokens.\n",
    "\n",
    "A central theme of the chapter is the tension between these frequency-based tokenization models and the unique, precise properties of source code. Unlike natural language, code is defined by its rigid syntax (e.g., \\= vs. \\==), semantically meaningful whitespace (e.g., Python's indentation), strict case sensitivity (userName vs. USERNAME), and reliance on special operators (-\\>, ::). Poor tokenization of these elements can lead to a fundamental loss of structural and semantic information before the model ever processes the input.\n",
    "\n",
    "The practical consequences of this tension are significant and manifest in two primary areas:\n",
    "\n",
    "1. **Efficiency and Cost:** Inefficient tokenization (fragmentation) inflates the token count, consuming the model's finite *context window* more rapidly. This directly translates to higher API costs, slower inference speeds, and a reduced \"token budget\" for a developer's prompts and any necessary retrieved context.  \n",
    "2. **Generation Quality:** Tokenization directly impacts the quality of the generated code. The chapter highlights how models may generate code with inconsistent operator spacing or favor shorter, more common variable names. This is often not a failure of the model's \"reasoning\" but a direct consequence of its tokenizer having fragmented these patterns, making them statistically easier to learn and reproduce.\n",
    "\n",
    "Finally, the chapter explores how the model's \"view\" of code is explicitly structured by *special tokens*. These include structural markers (\\<|endoftext|\\>), tokens for \"fill-in-the-middle\" tasks (\\<|fim\\_prefix|\\>, \\<|fim\\_suffix|\\>), and language-specific markers (\\<|python|\\>, \\<|javascript|\\>) that prime the model for a specific syntax.\n",
    "\n",
    "By understanding this tokenization layer, developers and engineers gain the ability to write more token-efficient prompts, debug non-obvious model failures, and better predict when a model will struggle or succeed, ultimately leading to a more effective application of LLMs in the software engineering lifecycle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8ffe91",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Glossary of Key Terms**\n",
    "\n",
    "* **Byte Pair Encoding (BPE):** A foundational subword tokenization algorithm. BPE begins with a base vocabulary of individual characters (or bytes) and iteratively learns a set of merge rules. In each step, it finds the *most frequently* occurring adjacent pair of tokens in the training corpus and merges them into a new, single token. This process is repeated until a desired vocabulary size is reached.  \n",
    "* **Context Window:** The finite and fixed number of tokens that an LLM can process at one time. This limit includes the input prompt, any retrieved context, and the generated output. Efficient tokenization is critical for maximizing the amount of information that can fit within this window.  \n",
    "* **SentencePiece:** A tokenization algorithm and software library that treats all input text, including whitespace, as a raw unicode sequence. Its key innovation is encoding whitespace as a special character (e.g., ), which allows it to tokenize and de-tokenize text reversibly without relying on language-specific pre-tokenization rules. It can be trained to use either BPE or Unigram models. \n",
    "* **Special Tokens:** A set of tokens reserved in the vocabulary to represent metadata, structural boundaries, or control signals rather than literal text. Examples from this chapter include :  \n",
    "  * \\<|endoftext|\\>: A token that marks the end of a document or logical text segment.  \n",
    "  * \\<|fim\\_prefix|\\>, \\<|fim\\_suffix|\\>, \\<|fim\\_middle|\\>: Tokens used in \"fill-in-the-middle\" (FIM) tasks, allowing the model to be trained to insert code between a given prefix and suffix.  \n",
    "  * \\<|python|\\>, \\<|javascript|\\>: Language-specific markers used to prime the model to generate code in a particular language.  \n",
    "  * \\\\n, \\\\t: Tokens that explicitly represent whitespace characters (newlines and tabs), which is critical for semantically meaningful indentation in languages like Python.  \n",
    "* **Subword Tokenization:** The dominant tokenization paradigm for modern LLMs. It serves as a compromise between *word-level* tokenization (which results in a massive vocabulary and fails to handle unknown words) and *character-level* tokenization (which has a small vocabulary but results in very long, inefficient token sequences). Subword algorithms break words into commonly occurring morphemes or \"subwords\".  \n",
    "* **Tokenization:** The process of converting a sequence of raw text (e.g., source code or natural language) into a sequence of discrete units called *tokens*.  \n",
    "* **Tokens:** The atomic elements that an LLM operates on. After tokenization, each token is mapped to a unique integer ID from a fixed vocabulary. The model reads, processes, and generates sequences of these token IDs.  \n",
    "* **Unigram:** A subword tokenization algorithm that operates in reverse of BPE. It starts with a very large vocabulary (e.g., all words and common substrings) and *prunes* tokens. It iteratively removes the (e.g., 10-20%) of tokens that *least affect the overall likelihood* of the training data according to a unigram language model, repeating until the target vocabulary size is reached.  \n",
    "* **WordPiece:** A subword tokenization algorithm used by models like BERT. It is similar to BPE, but instead of merging the most *frequent* pair, it merges the pair that *maximizes the likelihood* of the training data once merged. It essentially evaluates the \"loss\" of a merge to ensure it is statistically valuable.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "programming-genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
