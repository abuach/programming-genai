{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frontmatter \n",
    "\n",
    "This book offers a hands-on introduction to software engineering (and other related topics!) with generative AI, combining formal foundations with easily executable code, practical design patterns and best practices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract \n",
    "\n",
    "Something remarkable has happened in the world of programming. Large Language Models (LLMs) and generative AI have fundamentally changed what it means to write software. We've moved from telling computers exactly what to do, step by step, to showing them examples and watching them learn patterns we never explicitly programmed. It's a bit like the difference between following a recipe and learning to cook by watching someone who (hopefully!) knows what they're doing.\n",
    "\n",
    "What's also very exciting is that don't need a \"supercomputer\" anymore to get all the best benefits from this. The combination of open-source tools like Ollama, surprisingly capable small models, and the everyday laptop sitting on your desk has democratized generative AI in a way that would have seemed impossible just a few years ago. GenAI isn't a luxury reserved for big tech companies with massive GPU clusters—it's a **programmable resource** available to every developer, student, professor and curious tinkerer who wants to experiment.\n",
    "\n",
    "This book is a **hands-on, example-driven journey** through practical AI-augmented software engineering. We'll explore the core concepts you actually need: prompting or \"prompt engineering\" (the art of talking to models effectively), retrieval-augmented generation (teaching models to reference external knowledge), agent architectures (systems that can plan and execute multi-step tasks), and code generation/review that actually works. But here's the key—every concept comes with executable Python examples that you can run, modify, and learn from. No hand-waving, and nothing \"left as an exercise for the reader.\"\n",
    "\n",
    "We'll dig into the mechanics of how code-specific models work, why RAG systems help ground AI in reality rather than hallucinations, and how to design agents that can tackle complex programming tasks without getting lost. You'll understand not just *what* these systems do, but *how* they do it and *why* certain approaches work better than others.\n",
    "\n",
    "This material is written for teachers who want to bring AI into their curriculum, students building mental models of these strange new tools, and software engineers who want to integrate GenAI into their workflow. The goal is to make AI concrete and comprehensible, so you can use it productively and know when to trust it (and when not to).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on Book Software\n",
    "To aid reproducibility, I show the Python version — and versions of the major Python packages — used to build this executable book.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13.7\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This book makes significant use of Ollama and its Python API. Ollama is an open source tool that simplifies managing and running LLMs locally, like on a PC or local server, for example. Ollama executes models entirely on the user’s machine, providing full control over model selection, configuration, and execution. This local-first design makes Ollama especially valuable in *educational settings*, where transparency, reproducibility, and hands-on experimentation are essential.\n",
    "\n",
    "When using Ollama, students can work with modern open-weight models—such as general-purpose language models, code-focused models, and embedding models—without relying on external APIs or internet connectivity. This enables cost-free experimentation while preserving privacy and allowing direct observation of system behavior, including token usage, context limits, and output variability. Ollama’s simple command-line interface and API support interactive exploration and programmatic integration into software systems.\n",
    "\n",
    "In the college-level courses where this book is used, Ollama serves as a laboratory environment for studying generative AI as an engineered system rather than a black box. Working through guided labs, students explore core concepts such as prompt structure, sampling parameters, model tradeoffs, and retrieval-augmented generation. When they run models locally and embed them into their own applications, students begin to develop practical intuition about how large language models work and how they fail, their limitations and how they can be responsibly and effectively used in real production software systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ollama\n",
      "Version: 0.6.0\n",
      "Location: /Users/chikeabuah/Desktop/programming-genai/.venv/lib/python3.13/site-packages\n",
      "Requires: httpx, pydantic\n",
      "Required-by:\n"
     ]
    }
   ],
   "source": [
    "!uv pip show ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure and executable nature of this book is enabled by the [Jupyter](https://jupyter.org/) ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Jupyter core packages...\n",
      "IPython          : 9.6.0\n",
      "ipykernel        : 7.1.0\n",
      "ipywidgets       : 8.1.7\n",
      "jupyter_client   : 8.6.3\n",
      "jupyter_core     : 5.9.1\n",
      "jupyter_server   : 2.17.0\n",
      "jupyterlab       : 4.4.10\n",
      "nbclient         : 0.10.2\n",
      "nbconvert        : 7.16.6\n",
      "nbformat         : 5.10.4\n",
      "notebook         : 7.4.7\n",
      "qtconsole        : not installed\n",
      "traitlets        : 5.14.3\n"
     ]
    }
   ],
   "source": [
    "!jupyter --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This book is typset using Jupyter Book 2.0 which was in alpha release when I started writing this book.\n",
    "\n",
    "Jupyter Book is a great resource for sharing knowledge, and I am a big fan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.0.0-b3\n"
     ]
    }
   ],
   "source": [
    "!jupyter book --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.6.3\n"
     ]
    }
   ],
   "source": [
    "!jupyter kernel --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can personally recommend the [Microsoft Visual Studio Code](https://code.visualstudio.com/) editor for interacting with Jupyter Notebooks. It is free to use, and was used in the writing of this book (and its [predecessor](https://programming-dp.com/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.105.1\n",
      "7d842fb85a0275a4a8e4d7e040d2625abbf7f084\n",
      "arm64\n"
     ]
    }
   ],
   "source": [
    "!code --version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Co-Instructors: Our Models\n",
    "\n",
    "Not to get too anthropomorphic right off the bat, but to create an ideal educational experience we will need the perfect teaching assistants. \n",
    "\n",
    "Throughout this book we will make use of some of the most popular open-source GenAI models in the world at the time of initial writing (early 2026). No one model suits all purposes, and we will utilize about 6 or 7 different models that are uniquely specialized in some particular purpose like coding, vision, creative writing, creating embeddings, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       ID              SIZE      MODIFIED    \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    4 weeks ago    \n",
      "llama3.1:latest            46e0c10c039e    4.9 GB    6 weeks ago    \n",
      "llama3.2:latest            a80c4f17acd5    2.0 GB    6 weeks ago    \n",
      "qwen3:latest               500a1f067a9f    5.2 GB    6 weeks ago    \n",
      "qwen2.5-coder:latest       dae161e27b0e    4.7 GB    6 weeks ago    \n",
      "deepseek-r1:latest         6995872bfe4c    5.2 GB    6 weeks ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see above that I tend to utilize the default or `latest` category of model sizes, which is usually around the `~7b` parameter size. I used the same model sizes writing this book on my Macbook M4 32GB (when i say `GB` in this book I mean `gigabyte` by default) machine as my students used doing the labs on a shared Tesla P4 8GB gpu on the WWU CS LAN. \n",
    "\n",
    "I will briefly discuss each model in the following sections. I also show the download count at the time to emphasize each model's popularity in the open-source community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DeepSeek-R1` (75M Downloads)\n",
    "\n",
    "[`DeepSeek-R1`](https://ollama.com/library/deepseek-r1) is best understood as a reasoning-first model that prioritizes structured, multi-step thinking over speed or stylistic fluency. It performs especially well on mathematics, logic, and problems that benefit from explicit intermediate reasoning, often matching or exceeding larger generalist models on reasoning benchmarks. \n",
    "\n",
    "This comes at the cost of verbosity and latency: the model tends to produce longer responses and can feel slow or inefficient for simple queries. Its strengths make it well suited for research, theorem-style problem solving, and analytical tasks, but it is less compelling for casual dialogue, creative writing, or lightweight instruction following where faster, more concise models feel smoother.\n",
    "\n",
    "In my experience this model probably has less guardrails than the others mentioned here, and will speak it's \"mind\" on most topics. Definitely a great candidate for complex strategy, analysis, and ethics experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model\n",
      "    architecture        qwen3     \n",
      "    parameters          8.2B      \n",
      "    context length      131072    \n",
      "    embedding length    4096      \n",
      "    quantization        Q4_K_M    \n",
      "\n",
      "  Capabilities\n",
      "    completion    \n",
      "    thinking      \n",
      "\n",
      "  Parameters\n",
      "    stop           \"<｜begin▁of▁sentence｜>\"    \n",
      "    stop           \"<｜end▁of▁sentence｜>\"      \n",
      "    stop           \"<｜User｜>\"                 \n",
      "    stop           \"<｜Assistant｜>\"            \n",
      "    temperature    0.6                          \n",
      "    top_p          0.95                         \n",
      "\n",
      "  License\n",
      "    MIT License                    \n",
      "    Copyright (c) 2023 DeepSeek    \n",
      "    ...                            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ollama show deepseek-r1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Qwen2.5-Coder` (9.4M Downloads)\n",
    "\n",
    "[`Qwen2.5-Coder`](https://ollama.com/library/qwen2.5-coder) is a highly specialized model optimized almost entirely for programming tasks. It excels at code generation, refactoring, and debugging (code repair), often producing correct solutions on the first attempt and ranking near the top of open-source coding benchmarks. Its outputs tend to be structured, syntactically clean, and aligned with common software engineering patterns. \n",
    "\n",
    "However, this specialization also limits its versatility: outside of programming contexts, its reasoning, conversational quality, and creative abilities are noticeably weaker than strong generalist models. `Qwen2.5-Coder` is most effective when used as a dedicated coding assistant rather than a general AI system. This model is a good candidate to have as the default for a software engineering class. It is much more efficient in size at `5GB` for the `7b` (7 billion parameter version) than `Qwen3-Coder` which requires at least `19GB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model\n",
      "    architecture        qwen2     \n",
      "    parameters          7.6B      \n",
      "    context length      32768     \n",
      "    embedding length    3584      \n",
      "    quantization        Q4_K_M    \n",
      "\n",
      "  Capabilities\n",
      "    completion    \n",
      "    tools         \n",
      "    insert        \n",
      "\n",
      "  System\n",
      "    You are Qwen, created by Alibaba Cloud. You are a helpful assistant.    \n",
      "\n",
      "  License\n",
      "    Apache License               \n",
      "    Version 2.0, January 2004    \n",
      "    ...                          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ollama show qwen2.5-coder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Qwen3` (15.8M Downloads)\n",
    "\n",
    "[`Qwen3`](https://ollama.com/library/qwen3) is a strong general-purpose open model designed to balance reasoning ability, coding skill, multilingual coverage, tool-calling, and efficiency. It performs well across a wide range of benchmarks and supports different operating modes that trade off speed for deeper reasoning, making it flexible for both interactive use and harder analytical tasks. Its multilingual capabilities are particularly strong, and larger variants benefit from mixture-of-experts designs that improve efficiency without sacrificing quality. While generally competitive with the best open models, Qwen3 can occasionally struggle with strict instruction adherence or simple factual queries compared to more conservatively tuned models. Overall, it stands out as one of the most well-rounded open models available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model\n",
      "    architecture        qwen3     \n",
      "    parameters          8.2B      \n",
      "    context length      40960     \n",
      "    embedding length    4096      \n",
      "    quantization        Q4_K_M    \n",
      "\n",
      "  Capabilities\n",
      "    completion    \n",
      "    tools         \n",
      "    thinking      \n",
      "\n",
      "  Parameters\n",
      "    repeat_penalty    1                 \n",
      "    stop              \"<|im_start|>\"    \n",
      "    stop              \"<|im_end|>\"      \n",
      "    temperature       0.6               \n",
      "    top_k             20                \n",
      "    top_p             0.95              \n",
      "\n",
      "  License\n",
      "    Apache License               \n",
      "    Version 2.0, January 2004    \n",
      "    ...                          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ollama show qwen3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Gemma3` (28.8M Downloads)\n",
    "\n",
    "[`Gemma3`](https://ollama.com/library/gemma3) is a versatile, efficiency-oriented model family that is a good and creative writer in my experience. It supports text and vision inputs and performs reliably across many languages. The model strikes a balance between capability and hardware efficiency, with smaller variants that run comfortably on limited resources. While `Gemma3` is competitive on many benchmarks, it typically falls slightly behind the very top reasoning or coding-focused models at comparable scales. \n",
    "\n",
    "Its main appeal lies in its multimodal flexibility and strong all-around performance. I would recommend using this for classroom exercises that involve **computer vision**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model\n",
      "    architecture        gemma3    \n",
      "    parameters          4.3B      \n",
      "    context length      131072    \n",
      "    embedding length    2560      \n",
      "    quantization        Q4_K_M    \n",
      "\n",
      "  Capabilities\n",
      "    completion    \n",
      "    vision        \n",
      "\n",
      "  Parameters\n",
      "    stop           \"<end_of_turn>\"    \n",
      "    temperature    1                  \n",
      "    top_k          64                 \n",
      "    top_p          0.95               \n",
      "\n",
      "  License\n",
      "    Gemma Terms of Use                  \n",
      "    Last modified: February 21, 2024    \n",
      "    ...                                 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ollama show gemma3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Llama 3.2` (50.8M Downloads)\n",
    "\n",
    "[`Llama 3.2`](https://ollama.com/library/llama3.2) focuses on efficiency and is the smallest of our generalist models at `2GB` at `3b` parameters, which deserves special mention. The model is well tuned for conversational tasks, summarization, and general text understanding, delivering solid performance relative to its size. However, it does not match larger models on deep reasoning or advanced coding tasks. `Llama 3.2` is best seen as a very practical and deployment-friendly model for prototyping non-coding projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model\n",
      "    architecture        llama     \n",
      "    parameters          3.2B      \n",
      "    context length      131072    \n",
      "    embedding length    3072      \n",
      "    quantization        Q4_K_M    \n",
      "\n",
      "  Capabilities\n",
      "    completion    \n",
      "    tools         \n",
      "\n",
      "  Parameters\n",
      "    stop    \"<|start_header_id|>\"    \n",
      "    stop    \"<|end_header_id|>\"      \n",
      "    stop    \"<|eot_id|>\"             \n",
      "\n",
      "  License\n",
      "    LLAMA 3.2 COMMUNITY LICENSE AGREEMENT                 \n",
      "    Llama 3.2 Version Release Date: September 25, 2024    \n",
      "    ...                                                   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ollama show llama3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Llama 3.1` (108M Downloads)\n",
    "\n",
    "[`Llama 3.1`](https://ollama.com/library/llama3.1) represents Meta’s larger-scale generalist offering and is defined primarily by its breadth and scalability. Available in sizes ranging from modest `5GB` to extremely large `243GB`, it delivers strong performance across reasoning, coding, summarization, and dialogue, especially in its largest variants. The model supports long contexts and advanced features such as tool and function calling, making it suitable for complex agent-style applications. Its main drawback is resource intensity: the most capable versions require substantial compute and memory, limiting accessibility.\n",
    "\n",
    "Compared to `Llama 3.2`, it is less optimized for deployment but significantly stronger as a high-end, general-purpose language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model\n",
      "    architecture        llama     \n",
      "    parameters          8.0B      \n",
      "    context length      131072    \n",
      "    embedding length    4096      \n",
      "    quantization        Q4_K_M    \n",
      "\n",
      "  Capabilities\n",
      "    completion    \n",
      "    tools         \n",
      "\n",
      "  Parameters\n",
      "    stop    \"<|start_header_id|>\"    \n",
      "    stop    \"<|end_header_id|>\"      \n",
      "    stop    \"<|eot_id|>\"             \n",
      "\n",
      "  License\n",
      "    LLAMA 3.1 COMMUNITY LICENSE AGREEMENT            \n",
      "    Llama 3.1 Version Release Date: July 23, 2024    \n",
      "    ...                                              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ollama show llama3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nomic-embed-text` (48.5M Downloads) \n",
    "\n",
    "[`nomic-text-embed`](https://ollama.com/library/nomic-embed-text) is a text embedding model designed specifically for high-quality semantic representations rather than generative language tasks. Its primary strength lies in producing dense vector embeddings that capture meaning, similarity, and topical structure across documents, sentences, and short passages, making it well suited for classroom labs and projects dealing with retrieval-augmented generation (RAG), semantic search, clustering, and recommendation systems. \n",
    "\n",
    "The model emphasizes strong performance on retrieval and similarity benchmarks while remaining efficient enough for large-scale indexing. Because it is not a generative model, it does not produce natural language responses and is instead intended to be used as an infrastructure component that feeds downstream systems such as LLMs or search pipelines. \n",
    "\n",
    "Its appeal is in reliability, consistency, and alignment with modern vector-database workflows rather than conversational or reasoning capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model\n",
      "    architecture        nomic-bert    \n",
      "    parameters          137M          \n",
      "    context length      2048          \n",
      "    embedding length    768           \n",
      "    quantization        F16           \n",
      "\n",
      "  Capabilities\n",
      "    embedding    \n",
      "\n",
      "  Parameters\n",
      "    num_ctx    8192    \n",
      "\n",
      "  License\n",
      "    Apache License               \n",
      "    Version 2.0, January 2004    \n",
      "    ...                          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ollama show nomic-embed-text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "programming-genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
