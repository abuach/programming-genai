@article{dwork2008survey,
  title={Differential privacy: A survey of results},
  author={Dwork, Cynthia},
  journal={Theory and Applications of Models of Computation},
  pages={1--19},
  year={2008},
  publisher={Springer}
}

@inproceedings{mcsherry2007mechanism,
  title={Mechanism design via differential privacy},
  author={McSherry, Frank and Talwar, Kunal},
  booktitle={Foundations of Computer Science (FOCS)},
  year={2007},
  organization={IEEE}
}

@book{dwork2014algorithmic,
  title={The algorithmic foundations of differential privacy},
  author={Dwork, Cynthia and Roth, Aaron},
  year={2014},
  publisher={Foundations and Trends in Theoretical Computer Science}
}

@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
  pages={308--318},
  year={2016}
}

@inproceedings{bu2022deep,
  title={Deep learning with gaussian differential privacy},
  author={Bu, Zhiqi and Feldman, Vitaly and Hoory, Shuang and Talwar, Kunal and Thakurta, Abhradeep},
  booktitle={International Conference on Machine Learning},
  year={2022},
  organization={PMLR}
}

@inproceedings{shokri2017membership,
  title={Membership inference attacks against machine learning models},
  author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  booktitle={IEEE Symposium on Security and Privacy},
  year={2017}
}

@inproceedings{carlini2019secret,
  title={The secret sharer: Measuring unintended neural memorization},
  author={Carlini, Nicholas and Liu, Chang and Erlingsson, Úlfar and Kos, Jernej and Song, Dawn},
  booktitle={USENIX Security Symposium},
  year={2019}
}

@article{kasiviswanathan2011learnprivately,
  author    = {Shiva P. Kasiviswanathan and Homin K. Lee and Kobbi Nissim and Sofya Raskhodnikova and Adam Smith},
  title     = {What Can We Learn Privately?},
  journal   = {SIAM Journal on Computing},
  volume    = {40},
  number    = {3},
  pages     = {793--826},
  year      = {2011},
  doi       = {10.1137/090756090},
  url       = {https://doi.org/10.1137/090756090}
}

@inproceedings{sheffet2017dpols,
  author    = {Sheffet, Or},
  title     = {Differentially Private Ordinary Least Squares},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  pages     = {3105--3114},
  year      = {2017},
  editor    = {Precup, Doina and Teh, Yee Whye},
  month     = {Aug},
  publisher = {PMLR},
  url       = {https://proceedings.mlr.press/v70/sheffet17a.html},
  doi       = {10.5555/3305890.3306002}
}

@article{karwa2016noisydegrees,
  author    = {Vishesh Karwa and Aleksandra B. Slavkovi\'{c}},
  title     = {Inference using Noisy Degrees: Differentially Private $\beta$-Model and Synthetic Graphs},
  journal   = {The Annals of Statistics},
  volume    = {44},
  number    = {1},
  pages     = {87--112},
  year      = {2016},
  doi       = {10.1214/15-AOS1374},
  url       = {https://doi.org/10.1214/15-AOS1374}
}

@inproceedings{smith2011privacyestimation,
  author    = {Smith, Adam},
  title     = {Privacy‑Preserving Statistical Estimation with Optimal Convergence Rates},
  booktitle = {Proceedings of the 43rd ACM Symposium on Theory of Computing (STOC)},
  pages     = {813--822},
  year      = {2011},
  organization = {ACM},
  doi       = {10.1145/1993636.1993743},
  url       = {https://doi.org/10.1145/1993636.1993743}
}


@inproceedings{kairouz2015composition,
  author       = {Kairouz, Peter and Oh, Sewoong and Viswanath, Pramod},
  title        = {The Composition Theorem for Differential Privacy},
  booktitle    = {Proceedings of the 32nd International Conference on Machine Learning},
  series       = {Proceedings of Machine Learning Research},
  volume       = {37},
  pages        = {1376--1385},
  year         = {2015},
  editor       = {Francis Bach and David Blei},
  address      = {Lille, France},
  month        = jul,
  publisher    = {PMLR},
  url          = {https://proceedings.mlr.press/v37/kairouz15.html}
}

@techreport{uscensus2021das,
  author       = {{U.S. Census Bureau}},
  title        = {Disclosure Avoidance for the 2020 Census: An Introduction},
  institution  = {U.S.\ Government Publishing Office},
  address      = {Washington, DC},
  type         = {Handbook},
  month        = nov,
  year         = {2021},
  url          = {https://www2.census.gov/library/publications/decennial/2020/2020-census-disclosure-avoidance-handbook.pdf}
}

@techreport{abowd2022topdown,
  author       = {Abowd, John M. and Ashmead, Robert and Cumings-Menon, Ryan and Garfinkel, Simson and Heineck, Micah and Heiss, Christine and Johns, Robert and Kifer, Daniel and Leclerc, Philip and Machanavajjhala, Ashwin and Moran, Brett and Sexton, William and Spence, Matthew and Zhuravlev, Pavel},
  title        = {The 2020 Census Disclosure Avoidance System TopDown Algorithm},
  institution  = {U.S. Census Bureau},
  type         = {Working Paper},
  number       = {CED-WP-2022-002},
  month        = apr,
  year         = {2022},
  url          = {https://arxiv.org/abs/2204.08986}
}

@inproceedings{pierce2010distance,
author = {Reed, Jason and Pierce, Benjamin C.},
title = {Distance makes the types grow stronger: a calculus for differential privacy},
year = {2010},
isbn = {9781605587943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1863543.1863568},
doi = {10.1145/1863543.1863568},
abstract = {We want assurances that sensitive information will not be disclosed when aggregate data derived from a database is published. Differential privacy offers a strong statistical guarantee that the effect of the presence of any individual in a database will be negligible, even when an adversary has auxiliary knowledge. Much of the prior work in this area consists of proving algorithms to be differentially private one at a time; we propose to streamline this process with a functional language whose type system automatically guarantees differential privacy, allowing the programmer to write complex privacy-safe query programs in a flexible and compositional way.The key novelty is the way our type system captures function sensitivity, a measure of how much a function can magnify the distance between similar inputs: well-typed programs not only can't go wrong, they can't go too far on nearby inputs. Moreover, by introducing a monad for random computations, we can show that the established definition of differential privacy falls out naturally as a special case of this soundness principle. We develop examples including known differentially private algorithms, privacy-aware variants of standard functional programming idioms, and compositionality principles for differential privacy.},
booktitle = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming},
pages = {157–168},
numpages = {12},
keywords = {type systems, differential privacy},
location = {Baltimore, Maryland, USA},
series = {ICFP '10}
}

@article{abuah2022solo,
author = {Abuah, Chik\'{e} and Darais, David and Near, Joseph P.},
title = {Solo: a lightweight static analysis for differential privacy},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3563313},
doi = {10.1145/3563313},
abstract = {Existing approaches for statically enforcing differential privacy in higher order languages use either linear or relational refinement types. A barrier to adoption for these approaches is the lack of support for expressing these “fancy types” in mainstream programming languages. For example, no mainstream language supports relational refinement types, and although Rust and modern versions of Haskell both employ some linear typing techniques, they are inadequate for embedding enforcement of differential privacy, which requires “full” linear types. We propose a new type system that enforces differential privacy, avoids the use of linear and relational refinement types, and can be easily embedded in richly typed programming languages like Haskell. We demonstrate such an embedding in Haskell, demonstrate its expressiveness on case studies, and prove soundness of our type-based enforcement of differential privacy.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {150},
numpages = {30},
keywords = {Differential privacy, typechecking, verification}
}

@article{ullman2020overview,
  title={Privacy and data analysis: a research overview},
  author={Ullman, Jonathan},
  journal={ACM SIGACT News},
  year={2020}
}


@inproceedings{meyerson2004complexity,
  author    = {Adam Meyerson and Ryan Williams},
  title     = {On the Complexity of Optimal K-Anonymity},
  booktitle = {Proceedings of the 23rd ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS)},
  year      = {2004},
  pages     = {223--228},
  publisher = {ACM},
  doi       = {10.1145/1055558.1055591},
  url       = {https://doi.org/10.1145/1055558.1055591}
}

@book{rosenDiscreteMath,
  title     = {Discrete Mathematics and Its Applications},
  author    = {Rosen, Kenneth H.},
  publisher = {McGraw-Hill Education},
  year      = {2011},
  edition   = {7th},
  isbn      = {9780073383095}
}

@misc{lehmanMCS,
  title     = {Mathematics for Computer Science},
  author    = {Lehman, Eric and Leighton, Tom and Meyer, Albert R.},
  year      = {2017},
  url       = {https://people.csail.mit.edu/meyer/mcs.pdf}
}

@misc{levinDiscreteOpenIntro,
  title     = {Discrete Mathematics: An Open Introduction},
  author    = {Levin, Oscar},
  year      = {2023},
  edition   = {3rd},
  url       = {https://discrete.openmathbooks.org/}
}

@book{rossProbability,
  title     = {A First Course in Probability},
  author    = {Ross, Sheldon M.},
  publisher = {Pearson},
  year      = {2019},
  edition   = {10th},
  isbn      = {9780134753119}
}

@book{bertsekasProbability,
  title     = {Introduction to Probability},
  author    = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
  publisher = {Athena Scientific},
  year      = {2002},
  isbn      = {9781886529236}
}

@misc{grinsteadSnellProbability,
  title     = {Introduction to Probability},
  author    = {Grinstead, Charles M. and Snell, J. Laurie},
  year      = {2003},
  url       = {https://math.dartmouth.edu/~prob/prob/prob.pdf}
}

@misc{identifiability
, author = {Sweeney, Latanya}
, title={Simple Demographics Often Identify People Uniquely}
, url={https://dataprivacylab.org/projects/identifiability/}
, journal={Identifiability}}


@article{sweeney2002,
author = {Sweeney, Latanya},
title = {k-ANONYMITY: A MODEL FOR PROTECTING PRIVACY},
journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
volume = {10},
number = {05},
pages = {557-570},
year = {2002},
doi = {10.1142/S0218488502001648},
URL = {
        https://doi.org/10.1142/S0218488502001648
},
eprint = {
        https://doi.org/10.1142/S0218488502001648
}}

@inproceedings{mcsherry2009,
author = {McSherry, Frank D.},
title = {Privacy Integrated Queries: An Extensible Platform for Privacy-Preserving Data Analysis},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559850},
doi = {10.1145/1559845.1559850},
abstract = {We report on the design and implementation of the Privacy Integrated Queries (PINQ) platform for privacy-preserving data analysis. PINQ provides analysts with a programming interface to unscrubbed data through a SQL-like language. At the same time, the design of PINQ's analysis language and its careful implementation provide formal guarantees of differential privacy for any and all uses of the platform. PINQ's unconditional structural guarantees require no trust placed in the expertise or diligence of the analysts, substantially broadening the scope for design and deployment of privacy-preserving data analysis, especially by non-experts.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {19–30},
numpages = {12},
keywords = {differential privacy, linq, confidentiality, anonymization},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@InProceedings{dwork2006,
author="Dwork, Cynthia
and Kenthapadi, Krishnaram
and McSherry, Frank
and Mironov, Ilya
and Naor, Moni",
editor="Vaudenay, Serge",
title="Our Data, Ourselves: Privacy Via Distributed Noise Generation",
booktitle="Advances in Cryptology - EUROCRYPT 2006",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="486--503"
}

@inproceedings{dwork2006A,
author = {Dwork, Cynthia},
title = {Differential Privacy},
year = {2006},
isbn = {3540359079},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11787006_1},
doi = {10.1007/11787006_1},
abstract = {In 1977 Dalenius articulated a desideratum for statistical databases: nothing about an individual should be learnable from the database that cannot be learned without access to the database. We give a general impossibility result showing that a formalization of Dalenius' goal along the lines of semantic security cannot be achieved. Contrary to intuition, a variant of the result threatens the privacy even of someone not in the database. This state of affairs suggests a new measure, differential privacy, which, intuitively, captures the increased risk to one's privacy incurred by participating in a database. The techniques developed in a sequence of papers [8, 13, 3], culminating in those described in [12], can achieve any desired level of privacy under this measure. In many cases, extremely accurate information about the database can be provided while simultaneously ensuring very high levels of privacy},
booktitle = {Proceedings of the 33rd International Conference on Automata, Languages and Programming - Volume Part II},
pages = {1–12},
numpages = {12},
location = {Venice, Italy},
series = {ICALP'06}
}

@inproceedings{dwork2006B,
author = {Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
title = {Calibrating Noise to Sensitivity in Private Data Analysis},
year = {2006},
isbn = {3540327312},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11681878_14},
doi = {10.1007/11681878_14},
abstract = {We continue a line of research initiated in [10,11]on privacy-preserving statistical databases. Consider a trusted server that holds a database of sensitive information. Given a query function f mapping databases to reals, the so-called true answer is the result of applying f to the database. To protect privacy, the true answer is perturbed by the addition of random noise generated according to a carefully chosen distribution, and this response, the true answer plus noise, is returned to the user.Previous work focused on the case of noisy sums, in which f = ∑ig(xi), where xi denotes the ith row of the database and g maps database rows to [0,1]. We extend the study to general functions f, proving that privacy can be preserved by calibrating the standard deviation of the noise according to the sensitivity of the function f. Roughly speaking, this is the amount that any single argument to f can change its output. The new analysis shows that for several particular applications substantially less noise is needed than was previously understood to be the case.The first step is a very clean characterization of privacy in terms of indistinguishability of transcripts. Additionally, we obtain separation results showing the increased value of interactive sanitization mechanisms over non-interactive.},
booktitle = {Proceedings of the Third Conference on Theory of Cryptography},
pages = {265–284},
numpages = {20},
location = {New York, NY},
series = {TCC'06}
}

@inproceedings{nissim2007,
author = {Nissim, Kobbi and Raskhodnikova, Sofya and Smith, Adam},
title = {Smooth Sensitivity and Sampling in Private Data Analysis},
year = {2007},
isbn = {9781595936318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1250790.1250803},
doi = {10.1145/1250790.1250803},
abstract = {We introduce a new, generic framework for private data analysis.The goal of private data analysis is to release aggregate information about a data set while protecting the privacy of the individuals whose information the data set contains.Our framework allows one to release functions f of the data withinstance-based additive noise. That is, the noise magnitude is determined not only by the function we want to release, but also bythe database itself. One of the challenges is to ensure that the noise magnitude does not leak information about the database. To address that, we calibrate the noise magnitude to the smoothsensitivity of f on the database x --- a measure of variabilityof f in the neighborhood of the instance x. The new frameworkgreatly expands the applicability of output perturbation, a technique for protecting individuals' privacy by adding a smallamount of random noise to the released statistics. To our knowledge, this is the first formal analysis of the effect of instance-basednoise in the context of data privacy.Our framework raises many interesting algorithmic questions. Namely,to apply the framework one must compute or approximate the smoothsensitivity of f on x. We show how to do this efficiently for several different functions, including the median and the cost ofthe minimum spanning tree. We also give a generic procedure based on sampling that allows one to release f(x) accurately on manydatabases x. This procedure is applicable even when no efficient algorithm for approximating smooth sensitivity of f is known orwhen f is given as a black box. We illustrate the procedure by applying it to k-SED (k-means) clustering and learning mixtures of Gaussians.},
booktitle = {Proceedings of the Thirty-Ninth Annual ACM Symposium on Theory of Computing},
pages = {75–84},
numpages = {10},
keywords = {private data analysis, output perturbation, clustering, sensitivity, privacy preserving data mining},
location = {San Diego, California, USA},
series = {STOC '07}
}

@inproceedings{dwork2009,
author = {Dwork, Cynthia and Lei, Jing},
title = {Differential Privacy and Robust Statistics},
year = {2009},
isbn = {9781605585062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1536414.1536466},
doi = {10.1145/1536414.1536466},
abstract = {We show by means of several examples that robust statistical estimators present an excellent starting point for differentially private estimators. Our algorithms use a new paradigm for differentially private mechanisms, which we call Propose-Test-Release (PTR), and for which we give a formal definition and general composition theorems.},
booktitle = {Proceedings of the Forty-First Annual ACM Symposium on Theory of Computing},
pages = {371–380},
numpages = {10},
keywords = {propose-test-release paradigm, local sensitivity, differential privacy, robust statistics},
location = {Bethesda, MD, USA},
series = {STOC '09}
}

@article{dwork2014,
  title={The algorithmic foundations of differential privacy},
  author={Dwork, Cynthia and Roth, Aaron and others},
  journal={Foundations and Trends{\textregistered} in Theoretical Computer Science},
  volume={9},
  number={3--4},
  pages={211--407},
  year={2014},
  publisher={Now Publishers, Inc.}
}

@INPROCEEDINGS{dwork2010,
  author={Dwork, Cynthia and Rothblum, Guy N. and Vadhan, Salil},
  booktitle={2010 IEEE 51st Annual Symposium on Foundations of Computer Science},
  title={Boosting and Differential Privacy},
  year={2010},  volume={},  number={},  pages={51-60},  doi={10.1109/FOCS.2010.12}}

@inproceedings{bun2018composable,
  title={Composable and versatile privacy via truncated CDP},
  author={Bun, Mark and Dwork, Cynthia and Rothblum, Guy N and Steinke, Thomas},
  booktitle={Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing},
  pages={74--86},
  year={2018},
  organization={ACM}
}

@inproceedings{mironov2017renyi,
  title={Renyi differential privacy},
  author={Mironov, Ilya},
  booktitle={Computer Security Foundations Symposium (CSF), 2017 IEEE 30th},
  pages={263--275},
  year={2017},
  organization={IEEE}
}

@inproceedings{bun2016concentrated,
  title={Concentrated differential privacy: Simplifications, extensions, and lower bounds},
  author={Bun, Mark and Steinke, Thomas},
  booktitle={Theory of Cryptography Conference},
  pages={635--658},
  year={2016},
  organization={Springer}
}

@INPROCEEDINGS{mcsherry2007,
  author={McSherry, Frank and Talwar, Kunal},
  booktitle={48th Annual IEEE Symposium on Foundations of Computer Science (FOCS'07)},
  title={Mechanism Design via Differential Privacy},
  year={2007},  volume={},  number={},  pages={94-103},  doi={10.1109/FOCS.2007.66}}

@inproceedings{dwork2009A,
  author = {Dwork, Cynthia and Naor, Moni and Reingold, Omer and Rothblum, Guy N. and Vadhan, Salil},
  title = {On the Complexity of Differentially Private Data Release: Efficient Algorithms and Hardness Results},
  year = {2009},
  isbn = {9781605585062},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/1536414.1536467},
  doi = {10.1145/1536414.1536467},
  abstract = {We consider private data analysis in the setting in which a trusted and trustworthy curator, having obtained a large data set containing private information, releases to the public a "sanitization" of the data set that simultaneously protects the privacy of the individual contributors of data and offers utility to the data analyst. The sanitization may be in the form of an arbitrary data structure, accompanied by a computational procedure for determining approximate answers to queries on the original data set, or it may be a "synthetic data set" consisting of data items drawn from the same universe as items in the original data set; queries are carried out as if the synthetic data set were the actual input. In either case the process is non-interactive; once the sanitization has been released the original data and the curator play no further role.For the task of sanitizing with a synthetic dataset output, we map the boundary between computational feasibility and infeasibility with respect to a variety of utility measures. For the (potentially easier) task of sanitizing with unrestricted output format, we show a tight qualitative and quantitative connection between hardness of sanitizing and the existence of traitor tracing schemes.},
  booktitle = {Proceedings of the Forty-First Annual ACM Symposium on Theory of Computing},
  pages = {381–390},
  numpages = {10},
  keywords = {cryptography, privacy, differential privacy, traitor tracing, exponential mechanism},
  location = {Bethesda, MD, USA},
  series = {STOC '09}
  }

@inproceedings{rappor,
  author = {Erlingsson, \'{U}lfar and Pihur, Vasyl and Korolova, Aleksandra},
  title = {RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response},
  year = {2014},
  isbn = {9781450329576},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2660267.2660348},
  doi = {10.1145/2660267.2660348},
  abstract = {Randomized Aggregatable Privacy-Preserving Ordinal Response, or RAPPOR, is a technology for crowdsourcing statistics from end-user client software, anonymously, with strong privacy guarantees. In short, RAPPORs allow the forest of client data to be studied, without permitting the possibility of looking at individual trees. By applying randomized response in a novel manner, RAPPOR provides the mechanisms for such collection as well as for efficient, high-utility analysis of the collected data. In particular, RAPPOR permits statistics to be collected on the population of client-side strings with strong privacy guarantees for each client, and without linkability of their reports. This paper describes and motivates RAPPOR, details its differential-privacy and utility guarantees, discusses its practical deployment and properties in the face of different attack models, and, finally, gives results of its application to both synthetic and real-world data.},
  booktitle = {Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security},
  pages = {1054–1067},
  numpages = {14},
  keywords = {population statistics, crowdsourcing, cloud computing, statistical inference, privacy protection},
  location = {Scottsdale, Arizona, USA},
  series = {CCS '14}
  }

@article{warner1965,
  author = { Stanley L. Warner },
  title = {Randomized Response: A Survey Technique for Eliminating Evasive Answer Bias},
  journal = {Journal of the American Statistical Association},
  volume = {60},
  number = {309},
  pages = {63-69},
  year  = {1965},
  publisher = {Taylor & Francis},
  doi = {10.1080/01621459.1965.10480775},
  note ={PMID: 12261830},
  URL = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1965.10480775}}

@inproceedings {wang2017,
author = {Tianhao Wang and Jeremiah Blocki and Ninghui Li and Somesh Jha},
title = {Locally Differentially Private Protocols for Frequency Estimation},
booktitle = {26th {USENIX} Security Symposium ({USENIX} Security 17)},
year = {2017},
isbn = {978-1-931971-40-9},
address = {Vancouver, BC},
pages = {729--745},
url = {https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/wang-tianhao},
publisher = {{USENIX} Association},
month = aug,
}

@inproceedings{under2022,
author = {Casacuberta, Sílvia and Shoemate, Michael and Vadhan, Salil and Wagaman, Connor},
title = {Widespread Underestimation of Sensitivity in Differentially Private Libraries and How to Fix It},
year = {2022},
isbn = {9781450394505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548606.3560708},
doi = {10.1145/3548606.3560708},
abstract = {We identify a new class of vulnerabilities in implementations of differential privacy. Specifically, they arise when computing basic statistics such as sums, thanks to discrepancies between the implemented arithmetic using finite data types (namely, ints or floats) and idealized arithmetic over the reals or integers. These discrepancies cause the sensitivity of the implemented statistics (i.e., how much one individual's data can affect the result) to be much larger than the sensitivity we expect. Consequently, essentially all differential privacy libraries fail to introduce enough noise to hide individual-level information as required by differential privacy, and we show that this may be exploited in realistic attacks on differentially private query systems. In addition to presenting these vulnerabilities, we also provide a number of solutions, which modify or constrain the way in which the sum is implemented in order to recover the idealized or near-idealized bounds on sensitivity.},
booktitle = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
pages = {471–484},
numpages = {14},
keywords = {privacy attacks, finite-precision arithmetic, floating-point numbers, differential privacy},
location = {Los Angeles, CA, USA},
series = {CCS '22}
}

@inproceedings{mironov2012,
author = {Mironov, Ilya},
title = {On Significance of the Least Significant Bits for Differential Privacy},
year = {2012},
isbn = {9781450316514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2382196.2382264},
doi = {10.1145/2382196.2382264},
abstract = {We describe a new type of vulnerability present in many implementations of differentially private mechanisms. In particular, all four publicly available general purpose systems for differentially private computations are susceptible to our attack.The vulnerability is based on irregularities of floating-point implementations of the privacy-preserving Laplacian mechanism. Unlike its mathematical abstraction, the textbook sampling procedure results in a porous distribution over double-precision numbers that allows one to breach differential privacy with just a few queries into the mechanism.We propose a mitigating strategy and prove that it satisfies differential privacy under some mild assumptions on available implementation of floating-point arithmetic.},
booktitle = {Proceedings of the 2012 ACM Conference on Computer and Communications Security},
pages = {650–661},
numpages = {12},
keywords = {differential privacy, floating point arithmetic},
location = {Raleigh, North Carolina, USA},
series = {CCS '12}
} 


@InProceedings{pmlr-v80-balle18a,
  title = 	 {Improving the {G}aussian Mechanism for Differential Privacy: Analytical Calibration and Optimal Denoising},
  author =       {Balle, Borja and Wang, Yu-Xiang},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {394--403},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/balle18a/balle18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/balle18a.html},
  abstract = 	 {The Gaussian mechanism is an essential building block used in multitude of differentially private data analysis algorithms. In this paper we revisit the Gaussian mechanism and show that the original analysis has several important limitations. Our analysis reveals that the variance formula for the original mechanism is far from tight in the high privacy regime ($\varepsilon \to 0$) and it cannot be extended to the low privacy regime ($\varepsilon \to \infty$). We address these limitations by developing an optimal Gaussian mechanism whose variance is calibrated directly using the Gaussian cumulative density function instead of a tail bound approximation. We also propose to equip the Gaussian mechanism with a post-processing step based on adaptive estimation techniques by leveraging that the distribution of the perturbation is known. Our experiments show that analytical calibration removes at least a third of the variance of the noise compared to the classical Gaussian mechanism, and that denoising dramatically improves the accuracy of the Gaussian mechanism in the high-dimensional regime.}
}
