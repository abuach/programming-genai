{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb05cc3a",
   "metadata": {},
   "source": [
    "# Further Reading\n",
    "\n",
    "AI and large language models represent a convergence of decades of research in machine learning, natural language processing, computer systems, and human-computer interaction. This chapter highlights foundational papers, influential systems, and active research areas for readers who want to explore the topics we've covered in greater depth.\n",
    "\n",
    "## Foundations of Language Models\n",
    "\n",
    "- The transformer architecture that started it all:\n",
    "  - Vaswani et al. (2017): *Attention Is All You Need* {cite}`vaswani2017attention`\n",
    "  - Devlin et al. (2019): *BERT: Pre-training of Deep Bidirectional Transformers* {cite}`devlin2019bert`\n",
    "\n",
    "- Scaling laws and emergent capabilities:\n",
    "  - Kaplan et al. (2020): *Scaling Laws for Neural Language Models* {cite}`kaplan2020scaling`\n",
    "  - Wei et al. (2022): *Emergent Abilities of Large Language Models* {cite}`wei2022emergent`\n",
    "\n",
    "## Prompting and In-Context Learning\n",
    "\n",
    "- Understanding how LLMs learn from prompts:\n",
    "  - Brown et al. (2020): *Language Models are Few-Shot Learners* {cite}`brown2020language`\n",
    "  - Wei et al. (2022): *Chain-of-Thought Prompting Elicits Reasoning* {cite}`wei2022chain`\n",
    "  - Kojima et al. (2022): *Large Language Models are Zero-Shot Reasoners* {cite}`kojima2022large`\n",
    "\n",
    "- Prompt engineering techniques:\n",
    "  - Reynolds and McDonell (2021): *Prompt Programming for Large Language Models* {cite}`reynolds2021prompt`\n",
    "  - Zhou et al. (2023): *Large Language Models Are Human-Level Prompt Engineers* {cite}`zhou2023large`\n",
    "\n",
    "## Tokenization and Representation\n",
    "\n",
    "- Subword tokenization methods:\n",
    "  - Sennrich et al. (2016): *Neural Machine Translation of Rare Words with Subword Units* {cite}`sennrich2016neural`\n",
    "  - Kudo and Richardson (2018): *SentencePiece: A Simple and Language Independent Approach* {cite}`kudo2018sentencepiece`\n",
    "\n",
    "- Understanding embeddings:\n",
    "  - Mikolov et al. (2013): *Efficient Estimation of Word Representations in Vector Space* {cite}`mikolov2013efficient`\n",
    "  - Pennington et al. (2014): *GloVe: Global Vectors for Word Representation* {cite}`pennington2014glove`\n",
    "\n",
    "## Alignment and Safety\n",
    "\n",
    "- Reinforcement learning from human feedback:\n",
    "  - Christiano et al. (2017): *Deep Reinforcement Learning from Human Preferences* {cite}`christiano2017deep`\n",
    "  - Ouyang et al. (2022): *Training Language Models to Follow Instructions with Human Feedback* {cite}`ouyang2022training`\n",
    "\n",
    "- Understanding model behavior and safety:\n",
    "  - Bai et al. (2022): *Constitutional AI: Harmlessness from AI Feedback* {cite}`bai2022constitutional`\n",
    "  - Ganguli et al. (2023): *The Capacity for Moral Self-Correction in Large Language Models* {cite}`ganguli2023capacity`\n",
    "\n",
    "## Multimodal Models\n",
    "\n",
    "- Vision-language models:\n",
    "  - Radford et al. (2021): *Learning Transferable Visual Models From Natural Language Supervision* {cite}`radford2021learning`\n",
    "  - Ramesh et al. (2022): *Hierarchical Text-Conditional Image Generation with CLIP Latents* {cite}`ramesh2022hierarchical`\n",
    "\n",
    "- Unified architectures:\n",
    "  - Alayrac et al. (2022): *Flamingo: a Visual Language Model for Few-Shot Learning* {cite}`alayrac2022flamingo`\n",
    "\n",
    "## Model Efficiency and Compression\n",
    "\n",
    "- Making models faster and smaller:\n",
    "  - Hinton et al. (2015): *Distilling the Knowledge in a Neural Network* {cite}`hinton2015distilling`\n",
    "  - Frantar and Alistarh (2023): *SparseGPT: Massive Language Models Can Be Accurately Pruned* {cite}`frantar2023sparsegpt`\n",
    "\n",
    "- Low-rank adaptation:\n",
    "  - Hu et al. (2022): *LoRA: Low-Rank Adaptation of Large Language Models* {cite}`hu2022lora`\n",
    "\n",
    "## Retrieval Augmentation and Tool Use\n",
    "\n",
    "- Enhancing models with external knowledge:\n",
    "  - Lewis et al. (2020): *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks* {cite}`lewis2020retrieval`\n",
    "  - Borgeaud et al. (2022): *Improving Language Models by Retrieving from Trillions of Tokens* {cite}`borgeaud2022improving`\n",
    "\n",
    "- Tool use and agentic behavior:\n",
    "  - Schick et al. (2023): *Toolformer: Language Models Can Teach Themselves to Use Tools* {cite}`schick2023toolformer`\n",
    "  - Nakano et al. (2021): *WebGPT: Browser-assisted Question-Answering with Human Feedback* {cite}`nakano2021webgpt`\n",
    "\n",
    "## Privacy and Security\n",
    "\n",
    "- Privacy-preserving machine learning:\n",
    "  - Abadi et al. (2016): *Deep Learning with Differential Privacy* {cite}`abadi2016deep`\n",
    "  - McMahan et al. (2017): *Communication-Efficient Learning of Deep Networks from Decentralized Data* {cite}`mcmahan2017communication`\n",
    "\n",
    "- Adversarial robustness and attacks:\n",
    "  - Carlini et al. (2021): *Extracting Training Data from Large Language Models* {cite}`carlini2021extracting`\n",
    "  - Wallace et al. (2019): *Universal Adversarial Triggers for Attacking and Analyzing NLP* {cite}`wallace2019universal`\n",
    "\n",
    "## Interpretability and Mechanistic Understanding\n",
    "\n",
    "- Understanding how transformers work:\n",
    "  - Elhage et al. (2021): *A Mathematical Framework for Transformer Circuits* {cite}`elhage2021mathematical`\n",
    "  - Olsson et al. (2022): *In-Context Learning and Induction Heads* {cite}`olsson2022context`\n",
    "\n",
    "- Probing and analysis:\n",
    "  - Tenney et al. (2019): *BERT Rediscovers the Classical NLP Pipeline* {cite}`tenney2019bert`\n",
    "\n",
    "## Coding and Program Synthesis\n",
    "\n",
    "- Models for code generation:\n",
    "  - Chen et al. (2021): *Evaluating Large Language Models Trained on Code* {cite}`chen2021evaluating`\n",
    "  - Austin et al. (2021): *Program Synthesis with Large Language Models* {cite}`austin2021program`\n",
    "\n",
    "- Formal verification and correctness:\n",
    "  - Polu and Sutskever (2020): *Generative Language Modeling for Automated Theorem Proving* {cite}`polu2020generative`\n",
    "\n",
    "## Practical Resources and Libraries\n",
    "\n",
    "- Open-source frameworks:\n",
    "  - [Ollama](https://docs.ollama.com/)\n",
    "  - [UniXcoder](https://huggingface.co/microsoft/unixcoder-base)\n",
    "\n",
    "## Surveys and Perspectives\n",
    "\n",
    "- Comprehensive overviews:\n",
    "  - Zhao et al. (2023): *A Survey of Large Language Models* {cite}`zhao2023survey`\n",
    "  - Bommasani et al. (2021): *On the Opportunities and Risks of Foundation Models* {cite}`bommasani2021opportunities`\n",
    "\n",
    "- Future directions and open problems:\n",
    "  - Bubeck et al. (2023): *Sparks of Artificial General Intelligence: Early Experiments with GPT-4* {cite}`bubeck2023sparks`\n",
    "\n",
    "## References\n",
    "\n",
    "In the web/html version of the book, the bibliography will appear *directly below this current text section*.\n",
    "\n",
    "However in the print versions which are based on $\\LaTeX$, the bibliography will appear (more traditionally) as the penultimate un-numbered standalone chapter which precedes the Proof Index.\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrt\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
